---
title: 'Single-Model Demo: Generalized hqreg() + msaenet() Model'
author: "Matt Multach"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    toc: true
    toc_depth: 4
    toc_float: true
    number_sections: false
bibliography: Component_Refs.bib
#output: 
#       xaringan::infinite_moon_reader:
#              lib_dir: lib
#              highlightStyle: github
#              highlightLines: true
#              countIncrementalSlides: false
#              beforeInit: "macros.js"
#              css: [default, tamu, tamu-fonts]
---

## Introduction

This file demonstrates a model-application function which incorporates **<span style = "color:blue">`` `r "hqreg()"` ``</span>**-based models _and_ the multi-step adaptive elastic net from the *<span style = "color:red">`` `r "msaenet"` ``</span>* package through a singular model-application function. This walkthrough extends the work from 03A and 03B one step further to incorporate _both_ **<span style = "color:blue">`` `r "cv.hqreg()"` ``</span>** and **<span style = "color:blue">`` `r "msaenet()"` ``</span>**

Further mathematical and statistical details on the Adaptive LAD Lasso, Adaptive LAD Elastic Net, Adaptive Huber Lasso, Adaptive Huber Elastic Net, and Multi-Step Adaptive Elastic Net can be found in 00A_Lasso_ENet_Adaptations.

## Preamble and Setup

```{r setup, message = FALSE , warning = FALSE , include=FALSE}
knitr::opts_chunk$set(echo = TRUE , eval = TRUE)
```

### A note on formatting

This document makes use of **bolding**, _italics_, and "quotations" to help distinguish different types of items being referenced.

 * **bolding** will be used when referencing a new term or concept for the first time. Subsequent references to each term/concept will _not_ be bolded.
 * _italics_ will be used primarily for emphasis.
 * 'single quotations' will be used to clarify specific arguments for a function, or specific parameters of a mathemtical/statistical formulation
 * Inline references to code (functions, objects, specific commands, etc.) will use **<span style = "color:blue">`` `r "code_chunk_formatting"` ``</span>** in **<span style = "color:blue">`` `r "bolded blue font"` ``</span>**
 * Inline references to packages will similarly use *<span style = "color:red">`` `r "code_chunk_formatting"` ``</span>*, except in *<span style = "color:red">`` `r "italicized red font"` ``</span>*
 
Re: spacing and line breaks - I'm pretty heterogeneous in my application of line breaks and spacing, in a way that is idiosyncratic to my own code practice. The most important aspects of my spacing and line breaks are detailed below

I generally put spaces between code inputs I consider "sufficiently distinct". This improves readability generally, but I find it particularly helpful for debugging. Note, however, that spaces are generally trivial in between distinct code inputs in R, although this is not universally the case. Multi-character code inputs, such as the pointer **<span style = "color:blue">`` `r "<-"` ``</span>** and most logical operators, _cannot_ include spaces in between components of the code input. Note also that whitespace *is* meaningful in other programming languages, and so this convention should be considered with caution in your own practice.

Generally, I use line breaks to:

 * Break up separate arguments for a single command/function or chain of operations
 * To clearly distinguish between different closing parentheses, brackets, squigglies, etc., since RStudio will automatically tab each closing piece to match its opener.
 
### Packages

First, let's load the necessary packages. Links to more information about each packages and helpful guides (where applicable) can be found in 00B_Package_Descr_Refs. Appropriate references for each package can be found in the "References" section at the end of this document.

```{r libraries}
# This chunk loads the packages used in this workbook
library(xaringan)   # Allows active preview of report in RStudio
library(mvtnorm)    # Generates multivariate-normal data
library(magrittr)   # Used for piping
library(purrr)      # Used for mapping functions efficiently
library(data.table) # For more streamlined data structures
library(glmnet)     # For general lasso/elastic net functionality
library(hqreg)      # For LAD/Huber-loss regularization with SNCD algorithm
library(msaenet)    # For the multi-step adaptive elastic net
```

---
nocite: |
  @magrittr , @purrr , @xaringan , @mvtnorm , @data.table , @glmnet , @hqreg , @msaenet
---

## Data Loading

Let's load the singular dataset we created.

```{r load data , echo = T}
demo.data <- readRDS("/Users/Matt/Desktop/GitHub_Dissertation_Scripts/Robust_Lasso_ElasticNet/Datasets/TestingApplied_Xmtx.RData")
```

## Model-Application Function

### Data Input Formatting

Note that the function takes a list of data elements as one of its required (non-defaulted) arguments, and will read individual elements from that list named "X," "Y," "X_test," and "Y_test." If your data is not yet indexed for cross-validation training/testing, you can simply run the k-fold subsetting function **<span style = "color:blue">`` `r "kfold_subsetter"` ``</span>** described in 02A_KFold_Subsetter, setting the 'list' argument to 'list = "traintest" '.

If, however, you have a dataset already including a variable indicating training/testing subsets, you should run something like the following code chunk (which will not run if you compile this document, as 'eval = F'). You should replace inputs in ALLCAPS with the appropriate data object or cross-validation subset indicator in your dataset.

```{r split pre-indexed data for model function , eval = F}
data_for_hqreg <- list(X = OLDDATA[CV.INDEX != INDEX.VALUE.TESTSET , XCOL.NUMBERS] , 
                        Y = OLDDATA[CV.INDEX != INDEX.VALUE.TESTSET , YCOL.NUMBER] , 
                        X_test = OLDDATA[CV.INDEX == INDEX.VALUE.TESTSET , XCOL.NUMBERS] , 
                        Y_test = OLDDATA[CV.INDEX == INDEX.VALUE.TESTSET , YCOL.NUMBER])
```

### The Function

There are a few additional arguments beyond the function presented in 03A_AdaLAD_SingleDemo:
 
 * 'method' = tells the function which adaptation you would like to implement. You should specify one of the four when running the function on your own
   * "msaenet" is the multi-step adaptive elastic net described in @XiaoXu2015
   * "quantile" is just a generalization of LAD loss. I don't go over quantile-based regularization in these demos. If you are familiar with this type of regression and its use in this context, feel free to specify this and the corresponding argument 'tau' as you see fit
 * 'tau'/'gamma' = these arguments specify hyperparameters related to quantile loss and Huber loss, respectively
   * 'tau' defaults to 0.5 for quantile loss, which corresponds with LAD loss
   * 'gamma' defaults to 1.345 for Huber loss. Details on this choice can be found in 00A_Lasso_ENet_Adaptations, or from the original source, @Huber1981
   * NOTE: This "gamma" is NOT the same as the weighting hyperparameter used for adaptive weighting of the lasso tuning hyperparameter $\lambda_1$.
 * 'alpha' = balancing hyperparameter for contributions of lasso and ridge components to the overall elastic net model. Defaults to 0.5, which equates to an equal contribution of each regularization term
 * 'nsteps' = indicates the number of adaptive steps $k$ for the multi-step adaptive elastic net. Defaults to 5
 * 'print.time' = tells the function whether you want it print out the model run.time or not

```{r hqreg model application function , echo = T}

# adaptive LAD lasso application function
hqmsa.sim.fnct <- function(data.list , 
                           method = c("msaenet" , 
                                      "quantile" , "LAD" , 
                                      "huber") , 
                           tau = 0.5 , 
                           gamma = 1.345 , 
                           alpha = 0.5 , 
                           nsteps = 5L , 
                           print.time = TRUE) {
       # Store training X and Y to temporary objects
       X_train <- data.list[["X"]]
       Y_train <- data.list[["Y"]]
       
       if(method == "LAD") {
               method <- "quantile"
       }
       
       # If applicable, store holdout/testing X and Y
       if(!is.null(data.list[["X_test"]]) & 
          !is.null(data.list[["Y_test"]])) {
               X_test <- data.list[["X_test"]]
               Y_test <- data.list[["Y_test"]]
       } else {
               X_test <- NULL
               Y_test <- NULL
       }
        
       # lambdas to try for regularization
       lambda.try <- seq(log(1400) , log(0.01) , length.out = 100)
       lambda.try <- exp(lambda.try)
       
       # set a timer start point
       start <- Sys.time()
       
       # cross-validated selection of adaptive lasso
       # # tuning hyperparameter nu/gamma
       
       # # select ridge coefs for weighting
       ridge.model <- cv.glmnet(x = X_train , y = Y_train , 
                                lambda = lambda.try , alpha = 0)
       lambda.ridge.opt <- ridge.model$lambda.min
       best.ridge.coefs <- predict(ridge.model , 
                                           type = "coefficients" ,
                                           s = lambda.ridge.opt)[-1]

       
       # # grid of nu/gamma values to try for cross-validation
       nu.try <- exp(seq(log(0.01) , log(10) , length.out = 100))
       
       # # initialize full list of LAD lasso results from each nu/gamma
       hqmsa.nu.cv.full <- list()
       
       # # initialize matrices of metrics and minimizing results
       hqmsa.nu.cv.lambda <- numeric()
       hqmsa.nu.cv.mse <- numeric()
       hqmsa.nu.cv.msesd <- numeric()
       hqmsa.nu.cv.coefs <- list()
       
       # # Loop over nu/gamma values for CV, 
       # # # storing minimizing lambda within each nu/gamma
       if(method == "msaenet") {
               for(i in 1:length(nu.try)) {
                       #single adaptive lasso run with ridge weighting and nu = 1
                       hqmsa.nu.cv.full[[i]] <- msaenet(x = X_train , 
                                                    y = Y_train , 
                                                    family = "gaussian" , 
                                                    init = "ridge" ,
                                                    alphas = 0.5 , 
                                                    tune = "cv" , 
                                                    nfolds = 5L , 
                                                    rule = "lambda.min" , 
                                                    nsteps = nsteps , 
                                                    tune.nsteps = "max" , 
                                                    scale = nu.try[i])
                       
                       hqmsa.nu.cv.lambda[i] <-
                               hqmsa.nu.cv.full[[i]]$best.lambdas[[nsteps + 1]]
                       
                       hqmsa.nu.cv.coefs[[i]] <- c(NA , coef(hqmsa.nu.cv.full[[i]]))
                               
                       
                       hqmsa.nu.cv.mse[i] <- min(hqmsa.nu.cv.full[[i]]$step.criterion[[nsteps + 1]])
                       }
       } else {
               for(i in 1:length(nu.try)) {
                       invisible(capture.output(
                               hqmsa.nu.cv.full[[i]] <- 
                                       cv.hqreg(X = X_train , 
                                                y = Y_train , 
                                                method = method , 
                                                tau = tau , 
                                                gamma = gamma , 
                                                lambda = lambda.try ,
                                                alpha = alpha , 
                                                preprocess =
                                                        "standardize" , 
                                                screen = "ASR" , 
                                                penalty.factor = 
                                                        1 / abs(best.ridge.coefs) ^ nu.try[i] , 
                                                FUN = "hqreg" , 
                                                type.measure = "mse"
                                                )
                                        )
                               )
                       hqmsa.nu.cv.mse[i] <-
                               min(hqmsa.nu.cv.full[[i]]$cve)
                       hqmsa.nu.cv.msesd[i] <-
                               hqmsa.nu.cv.full[[i]]$cvse[
                         which.min(hqmsa.nu.cv.full[[i]]$cve)
                                                         ]
                       hqmsa.nu.cv.lambda[i] <-
                               hqmsa.nu.cv.full[[i]]$lambda.min
                       hqmsa.nu.cv.coefs[[i]] <- 
                               hqmsa.nu.cv.full[[i]]$fit$beta[ , 
                         which.min(hqmsa.nu.cv.full[[i]]$cve)
                                                             ]
                       }
       }

       
       #specify minimizing nu value and resulting model info
       nu.opt <- nu.try[which.min(hqmsa.nu.cv.mse)]
       lambda.opt <- 
               hqmsa.nu.cv.lambda[
                       which.min(hqmsa.nu.cv.mse)
                       ]
       weights.opt <- 1 / abs(best.ridge.coefs) ^ nu.opt
       hqmsa.coefs <-
               hqmsa.nu.cv.coefs[[
                       which.min(hqmsa.nu.cv.mse)
                       ]]
       hqmsa.mse.min <- min(hqmsa.nu.cv.mse)
       if(!is.null(hqmsa.nu.cv.msesd[1])) {
               hqmsa.mse.min.se <- hqmsa.nu.cv.msesd[
                       which.min(hqmsa.nu.cv.mse)
                       ]               
       }

       hqmsa.model.min <- 
               hqmsa.nu.cv.full[
                       which.min(hqmsa.nu.cv.mse)
                       ]
       n.coefs <- sum(hqmsa.coefs[-1] != 0)
       
       # calculate metrics using holdout data, if applicable
       if(!is.null(X_test) & !is.null(Y_test)) {
               # store n
               n <- nrow(data.list[["X_test"]])
               
               # calculate predicted values
               y.pred <- data.list[["X_test"]] %*% hqmsa.coefs[-1]
               if(!is.na(hqmsa.coefs[1])) {
                       y.pred <- y.pred + hqmsa.coefs[1]
               }
   
               # calculate residual
               resid <- y.pred - Y_test
               
               # square the residuals
               resid.sq <- resid ^ 2
               
               # sum the square of residuals
               sum.resid.sq <- sum(resid.sq)
               
               #calculate root mse
               mse <- sum.resid.sq / n
               
               # set endpoint for timer
               end <- Sys.time()
               
               # temporarily store time of current model
               time <- abs(as.numeric(difftime(start , 
                                               end , 
                                               units = "secs"
                                               )
                                      )
                           )
               
               # print the total runtime of the current model
               if(print.time) {
                       cat("time = " , time , " ;;; ")
               }
               
               
               # put conditions, model info, and metrics into list
               return(list(full.model = hqmsa.model.min ,
                           model.info = list(lambda = lambda.opt , 
                                             coefs = hqmsa.coefs , 
                                             weights = weights.opt
                                             ) , 
                           metrics = list(n.coefs = n.coefs , 
                                          runtime = time , 
                                          mse = mse
                                          )
                           )
                      )
       } else {
               # set endpoint for timer
               end <- Sys.time()
               
               # temporarily store time of current model
               time <- abs(as.numeric(difftime(start , 
                                               end , 
                                               units = "secs"
                                               )
                                      )
                           )
               
               # print the total runtime of the current model
               if(print.time) {
                       cat("time = " , time , " ;;; ")
               }
               
               # put conditions, model info, and metrics into list
               return(list(full.model = hqmsa.model.min ,
                           model.info = list(lambda = lambda.opt , 
                                             coefs = hqmsa.coefs , 
                                             weights = weights.opt
                                             ) , 
                           metrics = list(n.coefs = n.coefs , 
                                          runtime = time
                                          )
                           )
                      )
       }
       
       

       

}
```

### On Model Intercepts

Initially, this function was intended to give users the option of including or excluding a model intercept. However, the **<span style = "color:blue">`` `r "hqreg()"` ``</span>** family of functions inherently include an intercept in model estimation. Consequently, the function currently only estimates models with an intercept and produces corresponding metrics.

Unfortunately, the opposite is true for **<span style = "color:blue">`` `r "msaenet()"` ``</span>**: the function _only_ generates a model without an intercept. This function handles the discrepancy internally by adding an additional 'NA' element to the coefficient vectors for **<span style = "color:blue">`` `r "msaenet()"` ``</span>**. The calculation for generating MSE, if applicaple, handles both cases.

### On **<span style = "color:blue">`` `r "invisible(capture.output())"` ``</span>**

Astute readers might have noticed a pair of commands outside the use of **<span style = "color:blue">`` `r "cv.hqreg()"` ``</span>** above. This is to prevent an excess of model output getting printed, particularly from repeated model-application during the cross-validation process. **<span style = "color:blue">`` `r "cv.hqreg()"` ``</span>** will produce a large amount of output _for each cv data split_. Although the information is useful, having it printed out on-screen whenever you run the function is impractical, especially since we will ultimately be mapping this function over many datasets with one command.

## Function Testing

Now let's try running the model on our demo.data. Note that we're only running the function a single time and without any holdout data. Let's produce a 5-step multi-step adaptive elastic net.

```{r run adalad function}
msaenet5k.test <- hqmsa.sim.fnct(demo.data , 
                                  method = "msaenet")
```

Let's explore the structure of the resulting object.

```{r msaenet5k.test str}
str(msaenet5k.test)
```

The 2nd and 3rd list elements of the resulting object looks very similar to the one produced in 03A and 03B, although our first list element has many more pieces of information corresponding with the multi-step adaptive tuning process. The max depth of the full model object is 5 as a result. I encourage those who are familiar with regression model output in R, or with elastic net-type models, to explore the many elements contained in 'full.model'. However, the 'model.info' and 'metrics' list elements were created so that less-familiar and less-technically-oriented users can locate and examine the most relevant model information with ease.

The second list element contains the main model results:

 * The cv-selected lasso tuning hyperparameter $\lambda_1$
 * The resulting coefficients with names
 * The adaptive weighting vector applied to the resulting coefficients

The final list element contains metrics produced by the model-application function, which in this case were just the number of nonzero coefficients in the final model and the run.time of the model.

## References
