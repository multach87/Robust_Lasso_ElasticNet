---
title: 'Single-Model Demo: Adaptive LAD Lasso'
author: "Matt Multach"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    toc: true
    toc_depth: 4
    toc_float: true
    number_sections: false
bibliography: Component_Refs.bib
#output: 
#       xaringan::infinite_moon_reader:
#              lib_dir: lib
#              highlightStyle: github
#              highlightLines: true
#              countIncrementalSlides: false
#              beforeInit: "macros.js"
#              css: [default, tamu, tamu-fonts]
---

## Introduction

This file demonstrates application of the adaptive LAD lasso to a single dataset. The demo does not include holdout/testing data, although the function used to apply the adaptive LAD lasso incorporates the option.

Further mathematical and statistical details on the Adaptive LAD Lasso can be found in 00A_Lasso_ENet_Adaptations.

## Preamble and Setup

```{r setup, message = FALSE , warning = FALSE , include=FALSE}
knitr::opts_chunk$set(echo = TRUE , eval = TRUE)
```

### A note on formatting

This document makes use of **bolding**, _italics_, and "quotations" to help distinguish different types of items being referenced.

 * **bolding** will be used when referencing a new term or concept for the first time. Subsequent references to each term/concept will _not_ be bolded.
 * _italics_ will be used primarily for emphasis.
 * 'single quotations' will be used to clarify specific arguments for a function, or specific parameters of a mathemtical/statistical formulation
 * Inline references to code (functions, objects, specific commands, etc.) will use **<span style = "color:blue">`` `r "code_chunk_formatting"` ``</span>** in **<span style = "color:blue">`` `r "bolded blue font"` ``</span>**
 * Inline references to packages will similarly use *<span style = "color:red">`` `r "code_chunk_formatting"` ``</span>*, except in *<span style = "color:red">`` `r "italicized red font"` ``</span>*
 * References to other documents in this repository will use <span style = "color:green">`` `r "code_chunk_formatting"` ``</span> in <span style = "color:green">`` `r "un-italicized, un-bolded, green font"` ``</span>
 
Re: spacing and line breaks - I'm pretty heterogeneous in my application of line breaks and spacing, in a way that is idiosyncratic to my own code practice. The most important aspects of my spacing and line breaks are detailed below

I generally put spaces between code inputs I consider "sufficiently distinct". This improves readability generally, but I find it particularly helpful for debugging. Note, however, that spaces are generally trivial in between distinct code inputs in R, although this is not universally the case. Multi-character code inputs, such as the pointer **<span style = "color:blue">`` `r "<-"` ``</span>** and most logical operators, _cannot_ include spaces in between components of the code input. Note also that whitespace *is* meaningful in other programming languages, and so this convention should be considered with caution in your own practice.

Generally, I use line breaks to:

 * Break up separate arguments for a single command/function or chain of operations
 * To clearly distinguish between different closing parentheses, brackets, squigglies, etc., since RStudio will automatically tab each closing piece to match its opener.
 
### Packages

First, let's load the necessary packages. Links to more information about each packages and helpful guides (where applicable) can be found in <span style = "color:green">`` `r "00B_Package_Descr_Refs"` ``</span>. Appropriate references for each package can be found in the "References" section at the end of this document.

```{r libraries}
# This chunk loads the packages used in this workbook
library(xaringan)   # Allows active preview of report in RStudio
library(mvtnorm)    # Generates multivariate-normal data
library(magrittr)   # Used for piping
library(purrr)      # Used for mapping functions efficiently
library(data.table) # For more streamlined data structures
library(glmnet)     # For general lasso/elastic net functionality
library(hqreg)      # For LAD/Huber-loss regularization with SNCD algorithm
```

---
nocite: |
  @magrittr , @purrr , @xaringan , @mvtnorm , @data.table , @glmnet , @hqreg
---

## Data Loading

Let's load the singular, matricized dataset we created.

```{r load data , echo = T}
demo.data <- readRDS("/Users/Matt/Desktop/GitHub_Dissertation_Scripts/Robust_Lasso_ElasticNet/Datasets/TestingApplied_Xmtx.RData")
```

## Model-Application Function

### Data Input Formatting

Note that the function takes a list of data elements as its sole required argument, and will read individual elements from that list named "X," "Y," "X_test," and "Y_test." If your data is not yet indexed for cross-validation training/testing, you can simply run the k-fold subsetting function **<span style = "color:blue">`` `r "kfold_subsetter"` ``</span>** described in <span style = "color:green">`` `r "02A_KFold_Subsetter"` ``</span>, setting the 'list' argument to 'list = "traintest" '.

If, however, you have a dataset already including a variable indicating training/testing subsets, you should run something like the following code chunk (which will not run if you compile this document, as 'eval = F'). You should replace inputs in ALLCAPS with the appropriate data object or cross-validation subset indicator in your dataset.

```{r split pre-indexed data for model function , eval = F}
data_for_adalad <- list(X = OLDDATA[CV.INDEX != INDEX.VALUE.TESTSET , XCOL.NUMBERS] , 
                        Y = OLDDATA[CV.INDEX != INDEX.VALUE.TESTSET , YCOL.NUMBER] , 
                        X_test = OLDDATA[CV.INDEX == INDEX.VALUE.TESTSET , XCOL.NUMBERS] , 
                        Y_test = OLDDATA[CV.INDEX == INDEX.VALUE.TESTSET , YCOL.NUMBER])
```

### The Function

The function in the code chunk below applies the adaptive LAD lasso to specified data for predictors and a single response. Currently, the function only works for a single response Y and is not intended for use with multiple simultaneous response variables.

```{r ladlasso model application function , echo = T}

# adaptive LAD lasso application function
ladlasso.sim.fnct <- function(data.list) {
       # Store training X and Y to temporary objects
       X_train <- data.list[["X"]]
       Y_train <- data.list[["Y"]]
       
       # If applicable, store holdout/testing X and Y
       if(!is.null(data.list[["X_test"]]) & 
          !is.null(data.list[["Y_test"]])) {
               X_test <- data.list[["X_test"]]
               Y_test <- data.list[["Y_test"]]
       } else {
               X_test <- NULL
               Y_test <- NULL
       }
        
       # lambdas to try for regularization
       lambda.try <- seq(log(1400) , log(0.01) , length.out = 100)
       lambda.try <- exp(lambda.try)
       
       # set a timer start point
       start <- Sys.time()
       
       # cross-validated selection of adaptive lasso
       # # tuning hyperparameter nu/gamma
       
       # # select ridge coefs for weighting
       ridge.model <- cv.glmnet(x = X_train , y = Y_train , 
                                lambda = lambda.try , alpha = 0)
       lambda.ridge.opt <- ridge.model$lambda.min
       best.ridge.coefs <- predict(ridge.model , 
                                           type = "coefficients" ,
                                           s = lambda.ridge.opt)[-1]

       
       # # grid of nu/gamma values to try for cross-validation
       nu.try <- exp(seq(log(0.01) , log(10) , length.out = 100))
       
       # # initialize full list of LAD lasso results from each nu/gamma
       ladlasso.nu.cv.full <- list()
       
       # # initialize matrices of metrics and minimizing results
       ladlasso.nu.cv.lambda <- numeric()
       ladlasso.nu.cv.mse <- numeric()
       ladlasso.nu.cv.msesd <- numeric()
       ladlasso.nu.cv.coefs <- list()
       
       # # Loop over nu/gamma values for CV, 
       # # # storing minimizing lambda within each nu/gamma
       for(i in 1:length(nu.try)) {
         invisible(
                 capture.output(
                         ladlasso.nu.cv.full[[i]] <- 
                                 cv.hqreg(X = X_train , y = Y_train , 
                                          method = "quantile" , 
                                          tau = 0.5 , 
                                          lambda = lambda.try ,
                                          alpha = 1 , 
                                          preprocess = "standardize" , 
                                          screen = "ASR" , 
                                          penalty.factor = 
                                                  1 /
                                                  abs(best.ridge.coefs) ^
                                                  nu.try[i] , 
                                          FUN = "hqreg" , 
                                          type.measure = "mse"
                                          )
                         )
                 )
         ladlasso.nu.cv.mse[i] <- min(ladlasso.nu.cv.full[[i]]$cve)
         ladlasso.nu.cv.msesd[i] <-
                 ladlasso.nu.cv.full[[i]]$cvse[
                         which.min(ladlasso.nu.cv.full[[i]]$cve)
                         ]
         ladlasso.nu.cv.lambda[i] <- ladlasso.nu.cv.full[[i]]$lambda.min
         ladlasso.nu.cv.coefs[[i]] <- 
                 ladlasso.nu.cv.full[[i]]$fit$beta[ , 
                         which.min(ladlasso.nu.cv.full[[i]]$cve)
                         ]
       }
       
       #specify minimizing nu value and resulting model info
       nu.opt <- nu.try[which.min(ladlasso.nu.cv.mse)]
       lambda.opt <- 
               ladlasso.nu.cv.lambda[
                       which.min(ladlasso.nu.cv.mse)
                       ]
       weights.opt <- 1 / abs(best.ridge.coefs) ^ nu.opt
       ladlasso.coefs <-
               ladlasso.nu.cv.coefs[[
                       which.min(ladlasso.nu.cv.mse)
                       ]]
       ladlasso.mse.min <- min(ladlasso.nu.cv.mse)
       ladlasso.mse.min.se <-
               ladlasso.nu.cv.msesd[
                       which.min(ladlasso.nu.cv.mse)
                       ]
       ladlasso.model.min <- 
               ladlasso.nu.cv.full[
                       which.min(ladlasso.nu.cv.mse)
                       ]
       n.coefs <- sum(ladlasso.coefs[-1] != 0)
       
       # calculate metrics using holdout data, if applicable
       if(!is.null(X_test) & !is.null(Y_test)) {
               # store n
               n <- nrow(data.list[["X_test"]])
               
               # calculate predicted values
               y.pred <- (data.list[["X_test"]] %*% ladlasso.coefs[-1]) +
                               ladlasso.coefs[1]
               
               # calculate residual
               resid <- y.pred - Y_test
               
               # square the residuals
               resid.sq <- resid ^ 2
               
               # sum the square of residuals
               sum.resid.sq <- sum(resid.sq)
               
               #calculate root mse
               mse <- sum.resid.sq / n
               
               # set endpoint for timer
               end <- Sys.time()
               
               # temporarily store time of current model
               time <- abs(as.numeric(difftime(start , 
                                               end , 
                                               units = "secs"
                                               )
                                      )
                           )
               
               # print the total runtime of the current model
               cat("time = " , time , "\n")
               
               # put conditions, model info, and metrics into list
               return(list(full.model = ladlasso.model.min ,
                           model.info = list(lambda = lambda.opt , 
                                             coefs = ladlasso.coefs , 
                                             weights = weights.opt
                                             ) , 
                           metrics = list(mse = mse , 
                                          n.coefs = n.coefs , 
                                          runtime = time
                                  )
                           )
                      )
       } else {
               # set endpoint for timer
               end <- Sys.time()
               
               # temporarily store time of current model
               time <- abs(as.numeric(difftime(start , 
                                               end , 
                                               units = "secs"
                                               )
                                      )
                           )
               
               # print the total runtime of the current model
               cat("time = " , time , "\n")
               
               # put conditions, model info, and metrics into list
               return(list(full.model = ladlasso.model.min ,
                           model.info = list(lambda = lambda.opt , 
                                             coefs = ladlasso.coefs , 
                                             weights = weights.opt
                                             ) , 
                           metrics = list(n.coefs = n.coefs , 
                                          runtime = time
                                          )
                           )
                      )
       }
       
       

       

}
```


### On Model Intercepts

Initially, this function was intended to give users the option of including or excluding a model intercept. However, the **<span style = "color:blue">`` `r "hqreg()"` ``</span>** family of functions inherently include an intercept in model estimation. Consequently, the function currently only estimates models with an intercept and produces corresponding metrics.

### On **<span style = "color:blue">`` `r "invisible(capture.output())"` ``</span>**

Astute readers might have noticed a pair of commands outside the use of **<span style = "color:blue">`` `r "cv.hqreg()"` ``</span>** above. This is to prevent an excess of model output getting printed, particularly from repeated model-application during the cross-validation process. **<span style = "color:blue">`` `r "cv.hqreg()"` ``</span>** will produce a large amount of output _for each cv data split_. Although the information is useful, having it printed out on-screen whenever you run the function is impractical, especially since we will ultimately be mapping this function over many datasets with one command.

## Function Testing

Now let's try running the model on our demo.data. Note that we're only running the function a single time and without any holdout data.

```{r run adalad function}
ladlasso.test <- ladlasso.sim.fnct(demo.data)
```

Let's explore the structure of the resulting object.

```{r ladlasso.test str}
str(ladlasso.test)
```

A 3-element list is produced, each themselves containing various structures. The first list element contains the span of models produced **<span style = "color:blue">`` `r "cv.qreg()"` ``</span>** in the cross-validated selection of the lasso tuning hyperparameter $\lambda_1$, _for the cross-validation-selected minimizing scaling hyperparameter $\nu$ / $\gamma$_. The function itself generates a model for each potential value of this scaling hyperparameter, but ultimately only stores the minimizing model results (which are themselves a cross-validation-guided procedure for selecting another hyperparameter).

The second list element contains the main model results:

 * The cv-selected lasso tuning hyperparameter $\lambda_1$
 * The resulting coefficients with names
 * The adaptive weighting vector applied to the resulting coefficients

The final list element contains metrics produced by the model-application function, which in this case were just the number of nonzero coefficients in the final model and the run.time of the model.

## References