---
title: 'Combined-Model Generation'
author: "Matt Multach"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    toc: true
    toc_depth: 4
    toc_float: true
    number_sections: true
bibliography: ../References/Refs_Main.bib
#output: 
#       xaringan::infinite_moon_reader:
#              lib_dir: lib
#              highlightStyle: github
#              highlightLines: true
#              countIncrementalSlides: false
#              beforeInit: "macros.js"
#              css: [default, tamu, tamu-fonts]
---



# Introduction, Preamble, and Setup {.unnumbered .tabset .tabset-fade .tabset-pills}

```{r setup, message = FALSE , warning = FALSE , include=FALSE}
knitr::opts_chunk$set(echo = TRUE , eval = FALSE)
```

<style>
    .ref {
        position: relative;
        vertical-align: baseline;
    }

    .refnum {
        position: relative;
        left: -1px;
        bottom: 1ex;
        font-family: Verdana, sans-serif;
        color: #005994;
        font-size: .7em;
        font-weight: bold;
        text-decoration: underline;
        cursor: pointer;        
    }

    .refbody {
        font-family: Verdana, sans-serif;
        font-size: .7em;
        line-height: 1.1;
        display: none;
        min-width: 20em;
        position: absolute;
        left: 25px;
        bottom: 5px ;
        border: 1px solid;
        padding: 5px;
        background-color: #fff;
        word-wrap: break-word;
        z-index: 9999;
        overflow: auto;
    }

</style>

<script>
    function footypop(id) {
        var el = document.getElementById(id) ;
  
        if (el.style.display == "none") {
            el.style.display = "block" ;
        } else {
            el.style.display = "none" ;
        }
        
    }
</script>

## Introduction {.unnumbered}

The primary purpose of this file is to generate full model results for all adaptations and all 100 data splits and then save those models. I have set all chunks to 'eval = F,' as the model-mapping chunks would be incredibly time-intensive when compiling the .html file. However, I'm including this file and the corresponding .html to show the process which generated the full model results used in subsequent demos.

I'm going to be relatively sparse on details that have been covered in previous demos. I will mostly elaborate on the steps which generate and combine our full-model results.

## Formatting Notes {.unnumbered}

This document makes use of **bolding**, _italics_, and "quotations" to help distinguish references to different types of items.

 * **bolding** will be used when referencing a new term or concept for the first time. Subsequent references to each term/concept will _not_ be bolded.
 * _italics_ will be used primarily for emphasis.
 * 'single quotations' will be used to clarify specific arguments for a function, or specific parameters of a mathemtical/statistical formulation
 * Inline references to code (functions, objects, specific commands, etc.) will use **<span style = "color:blue">`` `r "code_chunk_formatting"` ``</span>** in **<span style = "color:blue">`` `r "bolded blue font"` ``</span>**
 * Inline references to packages will similarly use *<span style = "color:red">`` `r "code_chunk_formatting"` ``</span>*, except in *<span style = "color:red">`` `r "italicized red font"` ``</span>*
 * References to other documents in this repository will use <span style = "color:green">`` `r "code_chunk_formatting"` ``</span> in <span style = "color:green">`` `r "un-italicized, un-bolded, green font"` ``</span>
 
The most important aspects of how I use spacing and line breaks are detailed below.

I generally put spaces between code inputs I consider "sufficiently distinct". This improves readability generally, but I find it particularly helpful for debugging. Note, however, that spaces are generally trivial in between distinct code elements in R, although this is not universally the case. Multi-character code elements, such as the pointer **<span style = "color:blue">`` `r "<-"` ``</span>** and most logical operators, _cannot_ include spaces in between components of the element. Note also that whitespace *is* meaningful in other programming languages, and so this convention should be considered with caution in your own practice.

Generally, I use line breaks to:

 * Break up separate arguments for a single command/function or chain of operations
 * To clearly distinguish between different closing parentheses, brackets, squigglies, etc., since RStudio will automatically tab each closing piece to align with its opening segment.


## Packages {.unnumbered}

Let's load the necessary packages. Links to more information about each package can be found in <a href="https://multach87.github.io/Robust_Lasso_ElasticNet/Component_Demos/00B_Package_Descr_Refs.html"><span style = "color:green">`` `r "00B_Package_Descr_Refs"` ``</span></a>. Appropriate references for each package can be found in the "References" section at the end of this document.

```{r libraries , warning = FALSE}
# This chunk loads the packages used in this workbook
library(xaringan)   # Allows active preview of report in RStudio
library(mvtnorm)    # Generates multivariate-normal data
library(magrittr)   # Used for piping
library(purrr)      # Used for mapping functions efficiently
library(data.table) # For more streamlined data structures
library(glmnet)     # For general lasso/elastic net functionality
library(hqreg)      # For LAD/Huber-loss regularization with SNCD algorithm
library(rlang)      # For parse_expr() to parse data name in k-fold subsetting functions
library(msaenet)    # For the multi-step adaptive elastic net
```

## Loading and processing the data {.unnumbered .unlisted}

```{r load synthetic applied dataset}
demo.data <- readRDS("/Users/Matt/Desktop/GitHub_Dissertation_Scripts/Robust_Lasso_ElasticNet/Datasets/TestingApplied_Xmtx.RData")
```

```{r combine X and Y into single object}
demo.data2 <- setDT(data.frame(cbind(demo.data[["Y"]] , demo.data[["X"]]
                                     )
                               )
                    )
```

## CV-Splitting the Data {.unnumbered .unlisted}

### Custom k-fold subsetting function {.unnumbered .unlisted}

```{r kfold subsetting function , echo = F}
# k-fold subsetting function
kfold_subsetter <- function(data , y_col = 1 , 
                            x_cols = c(2:ncol(data)) , 
                            subset_col = (ncol(data) + 1) , 
                            k = 5 , seed = 7 , list = FALSE) {
        # check for string object in data argument
        if(is.character(data)) {
                data <- eval(parse_expr(data))
        }
        
        # check for data.table and setDT() if not a data.table
        if(!(TRUE %in% (class(data) == "data.table"))) {
                data <- setDT(data.frame(data))
        }
        
        # check for 0 < k <= n/2
        if((k <= 0) | (k > (nrow(data) / 2) ) ) {
                stop("ERROR: number of folds 'k' must be greater than 0 and less than or equal to half of the sample size")
        }
        
        # determine number of subsets which contain an extra observation
        # # if n is not evenly divisible by k
        # # # note that this value will be 0 if n/k is evenly divisible
        nsams.large <- nrow(data) %% k
        
        # determine number of smaller subset if n 
        # # is not evenly divisible by k
        # # # note that this will be the total number of samples if 
        # # # # n/k is evenly divisible
        nsams.small <- k - nsams.large
        
        # determine sample size of larger subsets if applicable
        samsize.large <- ceiling(nrow(data) / k) * (nsams.large != 0)
        
        # determine sample size of smaller/all subsets
        samsize.small <- floor(nrow(data) / k)
                
        # create indicator for each subset
        subset.indicator <- c(rep( (1 : k) , 
                                   floor(nrow(data) / k)
                                  ) ,
                              rep( (1 : (nsams.large) ) , 
                                   (1 * (nsams.large != 0) ) 
                                  )
                              )
                
        # fix random assignment process
        if(seed) {
                set.seed(seed)
        }
        
        # combine subset indicator with original data  
        newdata <- cbind(data , 
                         subset = sample(subset.indicator)
                         )
        
        # create k-split list if desired
        if(list == TRUE) {
                newdata <- return(split(newdata , 
                                        newdata[ , "subset"])
                                  )
        } else if(list == "traintest") {
                newdata <- return(list(
                    X = subset(newdata[ , c(x_cols, subset_col) , 
                                            with = F] , subset < k) %>%
                        .[ , c(1:length(x_cols)) , with = F] , #%>% 
                        #as.matrix() , 
                    Y = subset(newdata[ , c(y_col , subset_col) , 
                                        with = F] , subset < k) %>%
                        .[ , 1 , with = F] , #%>%
                        #as.matrix() , 
                    X_Test = subset(newdata[ , c(x_cols, subset_col) , 
                                            with = F] , subset %in% k) %>%
                        .[ , c(1:length(x_cols)) , with = F] , # %>%
                        #as.matrix() , 
                    Y_Test = subset(newdata[ , c(y_col , subset_col) , 
                                        with = F] , subset %in% k) %>%
                        .[ , 1 , with = F] , # %>%
                        #as.matrix() , 
                    Seed = seed , 
                    Subsets = newdata[ , "subset"]
                    )
                    )
        } else {
                newdata <- return(newdata)
        }
}



```

### Multi-Subsetting Wrapper {.unnumbered .unlisted}

```{r multi-kfold wrapper}
kfold_multi <- function(data , ... , seed_multi = 713 , 
                        num_splits = 100 , test_percent = .2) {
        # set the seed for generating individual kfold seeds
        set.seed(seed_multi)
        
        # turn data object name into character string for subsequent use
        data.name <- deparse(substitute(data))
        
        # generate random numbers equal to num_splits so that each
        # # individual run of the subsetter has a unique seed
        seeds <- sample(c(1:100000) , size = num_splits , replace = FALSE)
        
        # set value of k corresponding with test_percent
        if((test_percent >= 1) | (test_percent <= 0)) {
                stop("ERROR: 'test_percent' must be greater than and less than 1")
        }
        
        # set k for individual splits
        k <- 1 / test_percent
        
        # initialize and data.table of kfold arguments
        split_repped.dt <- setDT(as.data.frame(matrix(ncol = 1 , 
                                              nrow = num_splits)))
        
        # fill data.table with arguments for kfold function
        split_repped.dt[ , ':=' (data = data.name , 
                         k = k , 
                         seed = seeds , 
                         list = "traintest"
                         )
                 ]
        
        # remove blank column from initializing
        split_repped.dt <- split_repped.dt[ , !1 , with = F]
        
        # run subsetter for the desired number of splits
        full.data <- split_repped.dt %>%
                pmap(kfold_subsetter)
        
        # store single object of all training/testing splits
        return(full.data)
}
```


### Splitting the Data {.unnumbered .unlisted}

```{r splitting data}
demo.split <- list()

demo.split<- kfold_multi(demo.data2 , seed_multi = 999 , 
                         num_splits = 100 , test_percent = 0.2)
```










# Generating The Models {.tabset .tabset-fade .tabset-pills}

## Model-Application Function 

This is the full version of the model-application function, which can handle **<span style = "color:blue">`` `r "glmnet()"` ``</span>**, **<span style = "color:blue">`` `r "msaenet()"` ``</span>**, and **<span style = "color:blue">`` `r "hqreg()"` ``</span>**-based functions.

```{r hqreg model application function , echo = T}

# adaptive LAD lasso application function
hqmsa.sim.fnct <- function(data.list , 
                           method = c("msaenet" , 
                                      "quantile" , "LAD" , 
                                      "huber") , 
                           tau = 0.5 , 
                           gamma = 1.345 , 
                           alpha = 0.5 , 
                           nsteps = 5L , 
                           print.time = TRUE) {
       # Store training X and Y to temporary objects
       X_train <- as.matrix(data.list[["X"]])
       Y_train <- as.matrix(data.list[["Y"]])
       
       if(method == "LAD") {
               method <- "quantile"
       }
       
       # If applicable, store holdout/testing X and Y
       if(!is.null(data.list[["X_test"]]) & 
          !is.null(data.list[["Y_test"]])) {
               X_test <- as.matrix(data.list[["X_test"]])
               Y_test <- as.matrix(data.list[["Y_test"]])
       } else {
               X_test <- NULL
               Y_test <- NULL
       }
        
       # lambdas to try for regularization
       lambda.try <- seq(log(1400) , log(0.01) , length.out = 100)
       lambda.try <- exp(lambda.try)
       
       # set a timer start point
       start <- Sys.time()
       
       # cross-validated selection of adaptive lasso
       # # tuning hyperparameter nu/gamma
       
       # # select ridge coefs for weighting
       ridge.model <- cv.glmnet(x = X_train , y = Y_train , 
                                lambda = lambda.try , alpha = 0)
       lambda.ridge.opt <- ridge.model$lambda.min
       best.ridge.coefs <- predict(ridge.model , 
                                           type = "coefficients" ,
                                           s = lambda.ridge.opt)[-1]

       
       # # grid of nu/gamma values to try for cross-validation
       nu.try <- exp(seq(log(0.01) , log(10) , length.out = 100))
       
       # # initialize full list of LAD lasso results from each nu/gamma
       hqmsa.nu.cv.full <- list()
       
       # # initialize matrices of metrics and minimizing results
       hqmsa.nu.cv.lambda <- numeric()
       hqmsa.nu.cv.mse <- numeric()
       hqmsa.nu.cv.msesd <- numeric()
       hqmsa.nu.cv.coefs <- list()
       
       # # Loop over nu/gamma values for CV, 
       # # # storing minimizing lambda within each nu/gamma
       if(method == "msaenet") {
               for(i in 1:length(nu.try)) {
                       #single adaptive lasso run with ridge weighting and nu = 1
                       hqmsa.nu.cv.full[[i]] <- msaenet(x = X_train , 
                                                    y = Y_train , 
                                                    family = "gaussian" , 
                                                    init = "ridge" ,
                                                    alphas = 0.5 , 
                                                    tune = "cv" , 
                                                    nfolds = 5L , 
                                                    rule = "lambda.min" , 
                                                    nsteps = nsteps , 
                                                    tune.nsteps = "max" , 
                                                    scale = nu.try[i])
                       
                       hqmsa.nu.cv.lambda[i] <-
                               hqmsa.nu.cv.full[[i]]$best.lambdas[[nsteps + 1]]
                       
                       hqmsa.nu.cv.coefs[[i]] <- c(NA , coef(hqmsa.nu.cv.full[[i]]))
                               
                       
                       hqmsa.nu.cv.mse[i] <- min(hqmsa.nu.cv.full[[i]]$step.criterion[[nsteps + 1]])
                       }
       } else {
               for(i in 1:length(nu.try)) {
                       invisible(capture.output(
                               hqmsa.nu.cv.full[[i]] <- 
                                       cv.hqreg(X = X_train , 
                                                y = Y_train , 
                                                method = method , 
                                                tau = tau , 
                                                gamma = gamma , 
                                                lambda = lambda.try ,
                                                alpha = alpha , 
                                                preprocess =
                                                        "standardize" , 
                                                screen = "ASR" , 
                                                penalty.factor = 
                                                        1 / abs(best.ridge.coefs) ^ nu.try[i] , 
                                                FUN = "hqreg" , 
                                                type.measure = "mse"
                                                )
                                        )
                               )
                       hqmsa.nu.cv.mse[i] <-
                               min(hqmsa.nu.cv.full[[i]]$cve)
                       hqmsa.nu.cv.msesd[i] <-
                               hqmsa.nu.cv.full[[i]]$cvse[
                         which.min(hqmsa.nu.cv.full[[i]]$cve)
                                                         ]
                       hqmsa.nu.cv.lambda[i] <-
                               hqmsa.nu.cv.full[[i]]$lambda.min
                       hqmsa.nu.cv.coefs[[i]] <- 
                               hqmsa.nu.cv.full[[i]]$fit$beta[ , 
                         which.min(hqmsa.nu.cv.full[[i]]$cve)
                                                             ]
                       }
       }

       
       #specify minimizing nu value and resulting model info
       nu.opt <- nu.try[which.min(hqmsa.nu.cv.mse)]
       lambda.opt <- 
               hqmsa.nu.cv.lambda[
                       which.min(hqmsa.nu.cv.mse)
                       ]
       weights.opt <- 1 / abs(best.ridge.coefs) ^ nu.opt
       hqmsa.coefs <-
               hqmsa.nu.cv.coefs[[
                       which.min(hqmsa.nu.cv.mse)
                       ]]
       hqmsa.mse.min <- min(hqmsa.nu.cv.mse)
       if(!is.null(hqmsa.nu.cv.msesd[1])) {
               hqmsa.mse.min.se <- hqmsa.nu.cv.msesd[
                       which.min(hqmsa.nu.cv.mse)
                       ]               
       }

       hqmsa.model.min <- 
               hqmsa.nu.cv.full[
                       which.min(hqmsa.nu.cv.mse)
                       ]
       n.coefs <- sum(hqmsa.coefs[-1] != 0)
       
       # calculate metrics using holdout data, if applicable
       if(!is.null(X_test) & !is.null(Y_test)) {
               # store n
               n <- nrow(data.list[["X_test"]])
               
               # calculate predicted values
               y.pred <- data.list[["X_test"]] %*% hqmsa.coefs[-1]
               if(!is.na(hqmsa.coefs[1])) {
                       y.pred <- y.pred + hqmsa.coefs[1]
               }
   
               # calculate residual
               resid <- y.pred - Y_test
               
               # square the residuals
               resid.sq <- resid ^ 2
               
               # sum the square of residuals
               sum.resid.sq <- sum(resid.sq)
               
               #calculate root mse
               mse <- sum.resid.sq / n
               
               # set endpoint for timer
               end <- Sys.time()
               
               # temporarily store time of current model
               time <- abs(as.numeric(difftime(start , 
                                               end , 
                                               units = "secs"
                                               )
                                      )
                           )
               
               # print the total runtime of the current model
               if(print.time) {
                       cat("time = " , time , " ;;; ")
               }
               
               
               # put conditions, model info, and metrics into list
               return(list(full.model = hqmsa.model.min ,
                           model.info = list(lambda = lambda.opt , 
                                             coefs = hqmsa.coefs , 
                                             weights = weights.opt
                                             ) , 
                           metrics = list(n.coefs = n.coefs , 
                                          runtime = time , 
                                          mse = mse
                                          )
                           )
                      )
       } else {
               # set endpoint for timer
               end <- Sys.time()
               
               # temporarily store time of current model
               time <- abs(as.numeric(difftime(start , 
                                               end , 
                                               units = "secs"
                                               )
                                      )
                           )
               
               # print the total runtime of the current model
               if(print.time) {
                       cat("time = " , time , " ;;; ")
               }
               
               # put conditions, model info, and metrics into list
               return(list(full.model = hqmsa.model.min ,
                           model.info = list(lambda = lambda.opt , 
                                             coefs = hqmsa.coefs , 
                                             weights = weights.opt
                                             ) , 
                           metrics = list(n.coefs = n.coefs , 
                                          runtime = time
                                          )
                           )
                      )
       }
       
       

       

}
```

## Mapping 

The code chunks below map the model-application function **<span style = "color:blue">`` `r "hqmsa.sim.fnct()"` ``</span>** to our 100-split data. Each chunk generates 100 models from each of our different adaptations of interest using the arguments built into the function.

### Adaptive LAD Lasso {.unnumbered .unlisted}

```{r ladlasso}
#run across data subset
ladlasso.100 <- demo.split %>%
        map(safely(hqmsa.sim.fnct) , method = "LAD" , alpha = 1)
```

### Adaptive LAD Elastic Net, Alpha = 0.5 {.unnumbered .unlisted}

```{r ladelnet5}
#run across data subset
ladelnet5.100 <- demo.split %>%
        map(safely(hqmsa.sim.fnct) , method = "LAD" , alpha = 0.5)
```

### Adaptive Huber Lasso {.unnumbered .unlisted}

```{r huberlasso}
#run across data subset
huberlasso.100 <- demo.split %>%
        map(safely(hqmsa.sim.fnct) , method = "huber" , alpha = 1)
```

### Adaptive Huber Elastic Net, Alpha = 0.5 {.unnumbered .unlisted}

```{r huberelnet5}
#run across data subset
huberelnet5.100 <- demo.split %>%
        map(safely(hqmsa.sim.fnct) , method = "huber" , alpha = 0.5)
```

### Multi-Step Adaptive Elastic Net, k = 3, Alpha = 0.5 {.unnumbered .unlisted}

```{r msaenet k3}
#run across data subset
msaelnetk3.100 <- demo.split %>%
        map(safely(hqmsa.sim.fnct) , method = "msaenet" , nsteps = 3L)
```

### Multi-Step Adaptive Elastic Net, k = 5, Alpha = 0.5 {.unnumbered .unlisted}

```{r msaenet k5}
#run across data subset
msaelnetk5.100 <- demo.split %>%
        map(safely(hqmsa.sim.fnct) , method = "msaenet" , nsteps = 5L)
```

### Multi-Step Adaptive Elastic Net, k = 10, Alpha = 0.5 {.unnumbered .unlisted}

```{r msaenet k10}
#run across data subset
msaelnetk10.100 <- demo.split %>%
        map(safely(hqmsa.sim.fnct) , method = "msaenet" , nsteps = 10L)
```


# Processing the model results {.tabset .tabset-fade .tabset-pills}

Now we would like to process the resulting objects to eliminate any extra information. In this context, we're mainly interested in getting rid of the overarching 'result'/'error' list structure that is produced by **<span style = "color:blue">`` `r "safely()"` ``</span>**.




## Function for Processing Result/Error List elements

The following code chunk will check a data list produced by **<span style = "color:blue">`` `r "safely()"` ``</span>** and return either the 'result' element or the 'error' element, depending on which contains information.

```{r error or result function}
#eliminate result/error level
res_or_err <- function(data) {
        # if the 'error' list element is NULL, store the 'result' element contents
        if(is.null(data[["error"]])) {
                temp <- data[["result"]]
        } else { 
                # otherwise, store the 'error' element contents
                temp <- data[["error"]]
        }
        
        # save the resulting object
        return(temp)
}
```

## Get rid of result/error and combine

Now, we're going to map **<span style = "color:blue">`` `r "res_or_err()"` ``</span>** across all of our 100-split model results and combine those objects into a single overarching list.

```{r no resulterror and combine models , eval = F}
modelswt_all <- list(ladlasso = ladlasso.100 %>%
                       map(res_or_err) , 
                   ladelnet = ladelnet5.100 %>%
                       map(res_or_err) , 
                   huberlasso = huberlasso.100 %>%
                       map(res_or_err) , 
                   huberelnet = huberelnet5.100 %>%
                       map(res_or_err) , 
                   msak3 = msaelnetk3.100 %>%
                       map(res_or_err) , 
                   msak5 = msaelnetk5.100 %>%
                       map(res_or_err) ,
                   msak10 = msaelnetk10.100 %>%
                       map(res_or_err))
```




# References {.unnumbered .unlisted}

