---
title: 'Combined-Model Testing'
author: "Matt Multach"
date: "`r Sys.Date()`"
output: html_document
#output: 
#       xaringan::infinite_moon_reader:
#              lib_dir: lib
#              highlightStyle: github
#              highlightLines: true
#              countIncrementalSlides: false
#              beforeInit: "macros.js"
#              css: [default, tamu, tamu-fonts]
---

## Introduction

This file is used to test out the combined-model approach proposed in ADD DISSERTATION PROQUEST CITATION WHEN PUBLISHED.

## Preamble

```{r setup, message = FALSE , warning = FALSE , include=FALSE}
knitr::opts_chunk$set(echo = TRUE , eval = TRUE)
```

First, let's load the necessary packages. I've also set this code chunk to not print warnings, as they are largely not helpful for the current example.

```{r libraries , warning = FALSE}
# This chunk loads the packages used in this workbook
library(xaringan)   # Allows active preview of report in RStudio
library(mvtnorm)    # Generates multivariate-normal data
library(magrittr)   # Used for piping
library(purrr)      # Used for mapping functions efficiently
library(data.table) # For more streamlined data structures
library(glmnet)     # For general lasso/elastic net functionality
library(hqreg)      # For LAD/Huber-loss regularization with SNCD algorithm
library(rlang)      # For parse_expr() to parse data name in k-fold subsetting functions
library(msaenet)    # For the multi-step adaptive elastic net
```


Let's also load our test data, which will just be our matricized singular applied dataset.

```{r load synthetic applied dataset}
demo.data <- readRDS("/Users/Matt/Desktop/GitHub_Dissertation_Scripts/Robust_Lasso_ElasticNet/Datasets/TestingApplied_Xmtx.RData")
```

Let's also quickly combine the X and Y list elements into a single object for convenience with the subsetting function. We will put Y into the first column since it corresponds with a left-to-right reading of a regression model.

```{r combine X and Y into single object}
demo.data2 <- setDT(data.frame(cbind(demo.data[["Y"]] , demo.data[["X"]]
                                     )
                               )
                    )
```

## Loading custom functions

### Custom k-fold subsetting function

The chunk below takes a dataset and creates an ordinal variable with *k* levels, corresponding with each of the *k* test sets. The function takes as its arguments:

 * **data** : a dataset to be split into *k* groups
 * ***k*** : the number of folds for cross-validation
   * Defaults to 5 folds
   * Corresponds with the percentage of data to be used as a holdout/testing set in the following manner:
   $$
   \begin{equation}
        n_{test} = \text{
                $n_{full}$ $/$ $k$
        }
   \end{equation}
   $$
 * **seed** : a seed for fixing random processes
   * For this function, this is primarily associated with random and unordered assignment of the values of *k* across the full dataset
 * **list** : this argument tells the function if you want the resulting *k*-fold indexed data to be put into list format, with each list element corresponding with one fold *k*
   * This function defaults to *FALSE*, meaning the resulting data will be in matrix/tabular format and will contain a separate column for the index *k*
   * If set to *TRUE*, the resulting list will *not* contain the index variable for *k*, as the list elements themselves correspond with *k*
   * If set to *"traintest"* (in quotes!), the resulting list will instead contain two elements:
     * A training set containing data from *k* - 1 folds/subsets
     * A holdout/testing set containing data from the *k* fold/subset

```{r kfold subsetting function}
# k-fold subsetting function
kfold_subsetter <- function(data , y_col = 1 , 
                            x_cols = c(2:ncol(data)) , 
                            subset_col = (ncol(data) + 1) , 
                            k = 5 , seed = 7 , list = FALSE) {
        # check for string object in data argument
        if(is.character(data)) {
                data <- eval(parse_expr(data))
        }
        
        # check for data.table and setDT() if not a data.table
        if(!(TRUE %in% (class(data) == "data.table"))) {
                data <- setDT(data.frame(data))
        }
        
        # check for 0 < k <= n/2
        if((k <= 0) | (k > (nrow(data) / 2) ) ) {
                stop("ERROR: number of folds 'k' must be greater than 0 and less than sample size divided by 2")
        }
        
        # determine number of subsets which contain an extra element
        # # if n is not evenly divisible by k
        # # # note that this value will be 0 if n/k is evenly divisible
        nsams.large <- nrow(data) %% k
        
        # determine number of smaller subset if n 
        # # is not evenly divisible by k
        # # # note that this will be the total number of samples if n/k is evenly divisible
        nsams.small <- k - nsams.large
        
        # determine sample size of larger subsets if applicable
        samsize.large <- ceiling(nrow(data) / k) * (nsams.large != 0)
        
        # determine sample size of smaller/all subsets
        samsize.small <- floor(nrow(data) / k)
                
        # create indicator for each subset
        subset.indicator <- c(rep( (1 : k) , 
                                   floor(nrow(data) / k)
                                  ) ,
                              rep( (1 : (nsams.large) ) , 
                                   (1 * (nsams.large != 0) ) 
                                  )
                              )
                
        # fix random assignment process
        if(seed) {
                set.seed(seed)
        }
        
        # combine subset indicator with original data  
        newdata <- cbind(data , 
                         subset = sample(subset.indicator)
                         )
                
        newdata <- setDT(data.frame(newdata))
        
        # create k-split list if desired
        if(list == TRUE) {
                newdata <- return(split(newdata , 
                                        newdata[ , "subset"])
                                  )
        } else if(list == "traintest") {
                newdata <- return(list(
                    X = subset(newdata[ , c(x_cols, subset_col) , 
                                            with = F] , subset < k) %>%
                        .[ , c(1:8) , with = F] %>%
                        as.matrix() , 
                    Y = subset(newdata[ , c(y_col , subset_col) , 
                                        with = F] , subset < k) %>%
                        .[ , 1 , with = F] %>%
                        as.matrix() , 
                    X_Test = subset(newdata[ , c(x_cols, subset_col) , 
                                            with = F] , subset %in% k) %>%
                        .[ , c(1:8) , with = F] %>%
                        as.matrix() , 
                    Y_Test = subset(newdata[ , c(y_col , subset_col) , 
                                        with = F] , subset %in% k) %>%
                        .[ , 1 , with = F] %>%
                        as.matrix() , 
                    Seed = seed , 
                    Subsets = newdata[ , "subset"]
                        #Train = subset(newdata , subset < k) , 
                        #Test = subset(newdata , subset %in% k) , 
                        #Seed = seed
                        )
                        )
        } else {
                newdata <- return(newdata)
        }
}



```

### Multi-Subsetting Wrapper

The following function serves as a wrapper around the original subsetting function which incorporates additional arguments. These arguments extend the original function to conduct multiple training-testing splits and store the resulting datasets in a singular overarching list.

The wrapper's arguments are as follows. The ellipses are arguments for the original subsetting function which are handled internally within the wrapper.

 * **data** : a dataset to be split into training-testing sets
 * **seed_multi** : a seed for generating the vector of seeds for each individual training-testing *k*-fold split
 * **num_splits** : the number of training-testing splits to be created
    * Defaults to 100
 * **test_percent** : the percentage (as a decimal) of the full data to be reserved for the holdout/testing set
    * Corresponds with the number of folds *k* for *k*-fold subsetting in the following manner:
   $$
   \begin{equation}
        test\_percent = \text{
                $1$ $/$ $k$
        }
   \end{equation}
   $$
    * Defaults to 20$\%$, which corresponds with 5-fold subsetting

Note that this wrapper defaults to setting "list" argument in *kfold_subsetter()* to "traintest." This decision was implemented to reduce issues with formatting an applied dataset for the final combined-model function. The subsetting function is relatively more flexible in terms of data inputs than the full combined-model function, therefore I wanted to streamline data formatting from that point forward when using the combined model in full.

```{r multi-kfold wrapper}
kfold_multi <- function(data , ... , seed_multi = 713 , 
                        num_splits = 100 , test_percent = .2) {
        # set the seed for generating individual kfold seeds
        set.seed(seed_multi)
        
        # turn data object name into character string for subsequent use
        data.name <- deparse(substitute(data))
        
        # generate random numbers equal to num_splits so that each
        # # individual run of the subsetter has a unique seed
        seeds <- sample(c(1:100000) , size = num_splits , replace = FALSE)
        
        # set value of k corresponding with test_percent
        if((test_percent >= 1) | (test_percent <= 0)) {
                stop("ERROR: 'test_percent' must be greater than and less than 1")
        }
        
        # set k for individual splits
        k <- 1 / test_percent
        
        # initialize and data.table of kfold arguments
        split_repped.dt <- setDT(as.data.frame(matrix(ncol = 1 , 
                                              nrow = num_splits)))
        
        # fill data.table with arguments for kfold function
        split_repped.dt[ , ':=' (data = data.name , 
                         k = k , 
                         seed = seeds , 
                         list = "traintest"
                         )
                 ]
        
        # remove blank column from initializing
        split_repped.dt <- split_repped.dt[ , !1 , with = F]
        
        # run subsetter for the desired number of splits
        full.data <- split_repped.dt %>%
                pmap(kfold_subsetter)
        
        # store single object of all training/testing splits
        return(full.data)
}
```

### Model-Application Function

Finally, let's load the function used to generate our the models from our lasso and elastic net adaptations.

```{r hqreg model application function , echo = T}

# adaptive LAD lasso application function
hqmsa.sim.fnct <- function(data.list , 
                           method = c("msaenet" , 
                                      "quantile" , "LAD" , 
                                      "huber") , 
                           tau = 0.5 , 
                           gamma = 1.345 , 
                           alpha = 0.5 , 
                           nsteps = 10L , 
                           print.time = TRUE) {
       # Store training X and Y to temporary objects
       X_train <- data.list[["X"]]
       Y_train <- data.list[["Y"]]
       
       if(method == "LAD") {
               method <- "quantile"
       }
       
       # If applicable, store holdout/testing X and Y
       if(!is.null(data.list[["X_test"]]) & 
          !is.null(data.list[["Y_test"]])) {
               X_test <- data.list[["X_test"]]
               Y_test <- data.list[["Y_test"]]
       } else {
               X_test <- NULL
               Y_test <- NULL
       }
        
       # lambdas to try for regularization
       lambda.try <- seq(log(1400) , log(0.01) , length.out = 100)
       lambda.try <- exp(lambda.try)
       
       # set a timer start point
       start <- Sys.time()
       
       # cross-validated selection of adaptive lasso
       # # tuning hyperparameter nu/gamma
       
       # # select ridge coefs for weighting
       ridge.model <- cv.glmnet(x = X_train , y = Y_train , 
                                lambda = lambda.try , alpha = 0)
       lambda.ridge.opt <- ridge.model$lambda.min
       best.ridge.coefs <- predict(ridge.model , 
                                           type = "coefficients" ,
                                           s = lambda.ridge.opt)[-1]

       
       # # grid of nu/gamma values to try for cross-validation
       nu.try <- exp(seq(log(0.01) , log(10) , length.out = 100))
       
       # # initialize full list of LAD lasso results from each nu/gamma
       hqmsa.nu.cv.full <- list()
       
       # # initialize matrices of metrics and minimizing results
       hqmsa.nu.cv.lambda <- numeric()
       hqmsa.nu.cv.mse <- numeric()
       hqmsa.nu.cv.msesd <- numeric()
       hqmsa.nu.cv.coefs <- list()
       
       # # Loop over nu/gamma values for CV, 
       # # # storing minimizing lambda within each nu/gamma
       if(method == "msaenet") {
               for(i in 1:length(nu.try)) {
                       #single adaptive lasso run with ridge weighting and nu = 1
                       hqmsa.nu.cv.full[[i]] <- msaenet(x = X_train , 
                                                    y = Y_train , 
                                                    family = "gaussian" , 
                                                    init = "ridge" ,
                                                    alphas = 0.5 , 
                                                    tune = "cv" , 
                                                    nfolds = 5L , 
                                                    rule = "lambda.min" , 
                                                    nsteps = nsteps , 
                                                    tune.nsteps = "max" , 
                                                    scale = nu.try[i])
                       
                       hqmsa.nu.cv.lambda[i] <-
                               hqmsa.nu.cv.full[[i]]$best.lambdas[[nsteps + 1]]
                       
                       hqmsa.nu.cv.coefs[[i]] <- c(NA , coef(hqmsa.nu.cv.full[[i]]))
                               
                       
                       hqmsa.nu.cv.mse[i] <- min(hqmsa.nu.cv.full[[i]]$step.criterion[[nsteps + 1]])
                       }
       } else {
               for(i in 1:length(nu.try)) {
                       invisible(capture.output(
                               hqmsa.nu.cv.full[[i]] <- 
                                       cv.hqreg(X = X_train , 
                                                y = Y_train , 
                                                method = method , 
                                                tau = tau , 
                                                gamma = gamma , 
                                                lambda = lambda.try ,
                                                alpha = alpha , 
                                                preprocess =
                                                        "standardize" , 
                                                screen = "ASR" , 
                                                penalty.factor = 
                                                        1 / abs(best.ridge.coefs) ^ nu.try[i] , 
                                                FUN = "hqreg" , 
                                                type.measure = "mse"
                                                )
                                        )
                               )
                       hqmsa.nu.cv.mse[i] <-
                               min(hqmsa.nu.cv.full[[i]]$cve)
                       hqmsa.nu.cv.msesd[i] <-
                               hqmsa.nu.cv.full[[i]]$cvse[
                         which.min(hqmsa.nu.cv.full[[i]]$cve)
                                                         ]
                       hqmsa.nu.cv.lambda[i] <-
                               hqmsa.nu.cv.full[[i]]$lambda.min
                       hqmsa.nu.cv.coefs[[i]] <- 
                               hqmsa.nu.cv.full[[i]]$fit$beta[ , 
                         which.min(hqmsa.nu.cv.full[[i]]$cve)
                                                             ]
                       }
       }

       
       #specify minimizing nu value and resulting model info
       nu.opt <- nu.try[which.min(hqmsa.nu.cv.mse)]
       lambda.opt <- 
               hqmsa.nu.cv.lambda[
                       which.min(hqmsa.nu.cv.mse)
                       ]
       weights.opt <- 1 / abs(best.ridge.coefs) ^ nu.opt
       hqmsa.coefs <-
               hqmsa.nu.cv.coefs[[
                       which.min(hqmsa.nu.cv.mse)
                       ]]
       hqmsa.mse.min <- min(hqmsa.nu.cv.mse)
       if(!is.null(hqmsa.nu.cv.msesd[1])) {
               hqmsa.mse.min.se <- hqmsa.nu.cv.msesd[
                       which.min(hqmsa.nu.cv.mse)
                       ]               
       }

       hqmsa.model.min <- 
               hqmsa.nu.cv.full[
                       which.min(hqmsa.nu.cv.mse)
                       ]
       n.coefs <- sum(hqmsa.coefs[-1] != 0)
       
       # calculate metrics using holdout data, if applicable
       if(!is.null(X_test) & !is.null(Y_test)) {
               # store n
               n <- nrow(data.list[["X_test"]])
               
               # calculate predicted values
               y.pred <- data.list[["X_test"]] %*% hqmsa.coefs[-1]
               if(!is.na(hqmsa.coefs[1])) {
                       y.pred <- y.pred + hqmsa.coefs[1]
               }
   
               # calculate residual
               resid <- y.pred - Y_test
               
               # square the residuals
               resid.sq <- resid ^ 2
               
               # sum the square of residuals
               sum.resid.sq <- sum(resid.sq)
               
               #calculate root mse
               mse <- sum.resid.sq / n
               
               # set endpoint for timer
               end <- Sys.time()
               
               # temporarily store time of current model
               time <- abs(as.numeric(difftime(start , 
                                               end , 
                                               units = "secs"
                                               )
                                      )
                           )
               
               # print the total runtime of the current model
               if(print.time) {
                       cat("time = " , time , " ;;; ")
               }
               
               # put conditions, model info, and metrics into list
               return(list(full.model = hqmsa.model.min ,
                           model.info = list(lambda = lambda.opt , 
                                             coefs = hqmsa.coefs , 
                                             weights = weights.opt
                                             ) , 
                           metrics = list(n.coefs = n.coefs , 
                                          runtime = time , 
                                          mse = mse
                                          )
                           )
                      )
       } else {
               # set endpoint for timer
               end <- Sys.time()
               
               # temporarily store time of current model
               time <- abs(as.numeric(difftime(start , 
                                               end , 
                                               units = "secs"
                                               )
                                      )
                           )
               
               # print the total runtime of the current model
               if(print.time) {
                       cat("time = " , time , " ;;; ")
               }
               
               # put conditions, model info, and metrics into list
               return(list(full.model = hqmsa.model.min ,
                           model.info = list(lambda = lambda.opt , 
                                             coefs = hqmsa.coefs , 
                                             weights = weights.opt
                                             ) , 
                           metrics = list(n.coefs = n.coefs , 
                                          runtime = time
                                          )
                           )
                      )
       }
       
       

       

}
```

### Splitting the Data

Before we apply our models, let's create our data splits. We will use the multi-subsetting wrapper above to generate 100 random data splits. We will reserve 20$\%$ of the data for holdout/generating prediction error (aka, 5-fold subsets).

```{r splitting data}
demo.split <- list()

demo.split<- kfold_multi(demo.data2 , seed_multi = 999 , 
                         num_splits = 100 , test_percent = 0.2)
```

Here is a good place to stop and save the split data if you want store it for later use using the _saveRDS()_ function.

## Running our models

Now we can generate the models from our adaptations! I opted to generate the 100-model objects for each adaptation separately and then combined them. A little work with tools use throughout the different component demos could make a wrapper that generates all with one command and store them in a single objects. I will leave that as an exercise for anyone looking to practice function-writing. 

We are going to generate models from each of the following adaptations:

 * Adaptive LAD asso
 * Adaptive LAD elastic net, $\alpha = 0.5$
 * Adaptive Huber lasso, $M = 1.345$
 * Adaptive Huber elastic net, $M = 1.345, \alpha = 0.5$
 * Multi-step adaptive elastic net with $k = 3$ adaptive steps, $\alpha = 0.5$
 * Multi-step adaptive elastic net with $k = 5$ adaptive steps, $\alpha = 0.5$
 * Multi-step adaptive elastic net with $k = 10$ adaptive steps, $\alpha = 0.5$

For the purposes of testing, this demo only runs the models on 10 data splits each. The full demo of the combined-model approach will run each model across all 100 data splits.

### Adaptive LAD Lasso

```{r ladlasso}
#run across data subset
ladlasso.10 <- demo.split[1:10] %>%
        map(safely(hqmsa.sim.fnct) , method = "LAD" , alpha = 1)
```

### Adaptive LAD Elastic Net, Alpha = 0.5

```{r ladelnet5}
#run across data subset
ladelnet5.10 <- demo.split[1:10] %>%
        map(safely(hqmsa.sim.fnct) , method = "LAD" , alpha = 0.5)
```

### Adaptive Huber Lasso

```{r huberlasso}
#run across data subset
huberlasso.10 <- demo.split[1:10] %>%
        map(safely(hqmsa.sim.fnct) , method = "huber" , alpha = 1)
```

### Adaptive Huber Elastic Net, Alpha = 0.5

```{r huberelnet5}
#run across data subset
huberelnet5.10 <- demo.split[1:10] %>%
        map(safely(hqmsa.sim.fnct) , method = "huber" , alpha = 0.5)
```

### Multi-Step Adaptive Elastic Net, k = 3, Alpha = 0.5

```{r msaenet k3}
#run across data subset
msaelnetk3.10 <- demo.split[1:10] %>%
        map(safely(hqmsa.sim.fnct) , method = "msaenet" , nsteps = 3L)
```

### Multi-Step Adaptive Elastic Net, k = 5, Alpha = 0.5

```{r msaenet k5}
#run across data subset
msaelnetk5.10 <- demo.split[1:10] %>%
        map(safely(hqmsa.sim.fnct) , method = "msaenet" , nsteps = 5L)
```

### Multi-Step Adaptive Elastic Net, k = 10, Alpha = 0.5

```{r msaenet k10}
#run across data subset
msaelnetk10.10 <- demo.split[1:10] %>%
        map(safely(hqmsa.sim.fnct) , method = "msaenet" , nsteps = 10L)
```

### map(safely())

You may notice that in mapping each model across the first 10 data splits, we include an extra function _safely()_ within _map()_, in addition to the subsequent arguments to specify the adaptations of interest. If we look at the structure of one model from one adaptation

```{r str safely explain}
str(ladlasso.10[[1]] , max.level = 2)
```

we notice a list with 2 elements: 'result' and 'error', with 'result' containing 3 list elements. Using just _map()_ would have produced one of the following:
 * A list of 3 elements for each data split: _full.model_, _model.info_, and _metrics_
 * The mapping command would have failed due to an error

The _safely()_ function allows the mapping command to continue in the face of an error rather than stop completely. If an error had been produced by one of the models, the corresponding 'result' element would instead have been NULL. The 'error' element, on the other hand, would contain whatever error message was thrown while trying to map our model-application function.

Another note: in running _str()_ above, I specified an argument _max.level = 2_. _str()_ therefore only produced the structure to a depth of 2. If I had specified _max.level_, _str()_ would have given the structure for all objects down to the deepest level of object contained in _ladlasso.10_.

### Combine and save a temporary full model set

Just in case, I am going to combine all of the model files into one ugly dataset and save it.

```{r combine and save models , eval = F}
models_combine <- list(ladlasso = ladlasso.10 , ladelnet = ladelnet5.10 , 
                       huberlasso = huberlasso.10 , huberelnet = huberelnet5.10 , 
                       msak3 = msaelnetk3.10 , msak5 = msaelnetk5.10 , 
                       msak10 = msaelnetk10.10)

saveRDS(models_combine , "/Users/Matt/Desktop/GitHub_Dissertation_Scripts/Robust_Lasso_ElasticNet/models_combine10.RData")
```


## Processing the model results

There are a few ways we could process and organize the model results. There 2 main steps of processing that need to occur:
 * Move either the "result" or "error" list elements from each model up a level in the list
 * Put the model results from the different adaptations into a single combined object

### Get rid of result/error level of each model: Function

Fun fact: the _ifelse()_ results in an object with the same dimensions as the input object. Our input data will be the individual model list elements, corresponding with the result/error list that the model produced for each data split. Unfortunately, the results of each model are lists with 3 elements. Consequently, the code chunk below will only store the first of the 3 elements that we want stored.

```{r ifelse error result function , eval = F}
res_or_err <- function(data) {
    return(ifelse(is.null(data[["error"]] , data[["result"]] , data[["error"]])
                  )
           )
}
```

I didn't know this before working on this demo! As a result of this interesting behavior, the function which will be mapped onto each individual model will instead utilize separate if-else statements.

```{r error or result function}
#eliminate result/error level
res_or_err <- function(data) {
        if(is.null(data[["error"]])) {
                temp <- data[["result"]]
        } else {
                temp <- data[["error"]]
        }
        
        return(temp)
}
```

### Get rid of result/error and combine

```{r no resulterror and combine models}
models_all <- list(ladlasso = ladlasso.10 %>%
                       map(res_or_err) , 
                   ladelnet = ladelnet5.10 %>%
                       map(res_or_err) , 
                   huberlasso = huberlasso.10 %>%
                       map(res_or_err) , 
                   huberelnet = huberelnet5.10 %>%
                       map(res_or_err) , 
                   msak3 = msaelnetk3.10 %>%
                       map(res_or_err) , 
                   msak5 = msaelnetk5.10 %>%
                       map(res_or_err) ,
                   msak10 = msaelnetk10.10 %>%
                       map(res_or_err))
```

### Save processed full object

```{r save processed models}
saveRDS(models_all , "/Users/Matt/Desktop/GitHub_Dissertation_Scripts/Robust_Lasso_ElasticNet/Datasets/models10.RData")
```

## Processing Coefficients

Before we actually look at and visualize the results of our models, we need to process the coefficients so that they're in a practical form.

### Pre-Processing Organization

First, though, let's clear our environment and then load the 100-split model file, which was generated separately to save time for readers.

```{r clear environment prior to coefficient processing}
rm(list = ls())
```

```{r load 100-split model RData}
models_all <- readRDS("/Users/Matt/Desktop/GitHub_Dissertation_Scripts/Robust_Lasso_ElasticNet/Datasets/models100.RData")
```

### Coefficient frequency function

There are many pieces of information that might be extracted from such a rich collection of model results. However, the combined model that I am working towards is particularly interested in selection frequency for each of the potential predictors. 

I'd like to add a list element that contains information about the models produced by each adaptation across all data splits, particularly with respect to coefficient inclusion frequency. Note that this does *not* include intercepts, given that some models do not include them.

Also note that this function relies on a list with a comparable structure to my processed models:
 * Level 1 of list corresponds with each data split
 * Level 2 of list corresponds with different model result objects from a given data split
   * One list element at Level 2 contains the model summary object _model.info_
     *which contains a list element _coefs_, a numeric vector of coefficient values (and intercept, if applicable)

```{r extract coefficient information}
coefs_freq <- function(models) {
       # initialize vector of predictor selection frequency, with all values set to 0
       coefs.freq <- numeric(length = 8)
       
       # label frequency vector with predictor names, accounting for 
       names(coefs.freq) <- names(models[[1]][["model.info"]][["coefs"]][-1])
       
       # update each predictors' selection frequency if the corresponding coefficient value
       # # in each data split is nonzero (aka, was selected into the model)
       for(i in 1:length(models)) {
              for(j in 1:length(coefs.freq)) {
                     if(models[[i]][["model.info"]][["coefs"]][(j + 1)] != 0) {
                            coefs.freq[j] <- coefs.freq[j] + 1
                     }
              }
       }
       
       return(coefs.freq)       
}
```

Now let's map our coefficient frequency function across all adaptations and model results.

### Minor functions

I'd also like to count the number of variables with $100\%$ or $0\%$ selection for each adaptation. I'm going to create two basic functions that I can apply during piping.

```{r all_splits function}
all_splits <- function(data) {
        return(length(which(data[1:8] == 100)
                      )
               )
}
```

```{r no_splits function}
no_splits <- function(data) {
        return(length(which(data[1:8] == 0)
                      )
        )
}
```

Similarly, I would also like to calculate the number of adaptations which selected each variable $100\%$ and $0\%$ of the time.

```{r adapts_100 function}
adapts_100 <- function(data) {
        return(length(which(data[1:7] == 100)
                      )
               )
}
```

```{r adapts_0 function}
adapts_0 <- function(data) {
        return(length(which(data[1:7] == 0)
                      )
        )
}
```

### Coefficient processing: Initialize list

Our original _coefs_freq()_ function creates a named, $p$-length vector of frequencies of predictor selection for a single lasso or elastic net adaptation. Here, we need to get these frequencies for each of our 7 adaptations. We also want the resulting information to be stored and located conveniently. We also want to create a similar table of selection info, except by potential predictor instead of by adaptation.

Our new list element, "Frequencies", will have two data.tables:
 * A data.table of selection frequencies by adaptation (rows = lasso/elastic net adaptation)
 * A data.table of selection frequencies by potential predictor (rows = potential predictor X1 - X8) 

####UPDATE THIS SECTION/MOVE TO APPROPRIATE LOCATION
Let's map this function across each of the 7 list elements in _models_all_ corresponding with each of our 7 adaptations. Re: storage and location of the coefficient frequences, let's:
  * Store all 8-length frequency vectors in a singular, $7 x 8$ data.table
    * Store this data.table in a new list element in _models_all_ named _"Frequencies"_ and a data.table within that list named _"By_Adaptation"_
  * Add a 9th column with the number of variables selected into the model in all 100 data splits
  * Add a 10th column with the number of variables that was unselected in all 100 data splits
  * Add a final column identifying the method the generated row of selection frequencies

####UPDATE THIS SECTION/MOVE TO APPROPRIATE LOCATION  
Let's also map this function across each of the 7 list elements in _models_all_ corresponding with each of our 8 potential predictors. Re: storage and location of the coefficient frequences, let's:



Let's initialize that list.

```{r}
models_all[["Frequencies"]] <- list()
```


### Coefficient frequency processing: By Adaptation

Now let's map our coefficient frequency function across all adaptations and model results. to create our first data.table.

```{r run coef frequency function on ladlasso models}
models_all[["Frequencies"]][["By_Adaptation"]] <- models_all[1:7] %>%
         map(coefs_freq) %>%
         setDT %>%
         t %>%
         data.frame %>%
         setDT %>%
         .[ , "all" := apply(. , 1 , all_splits)] %>%
         .[ , "none" := apply(. , 1 , no_splits)] %>%
         .[ , "method" := names(models_all[-8])] %>%
         setkey(. , method) 
```

Let's walk through what's going on in the last code chunk. 
  * First, establish reference to the _models_all_ object for the subsequent pipe operations
  * Second, _map()_ the _coefs_freq()_ function across elements of _models_all_
  * Third, make the resulting object a data.table using _setDT()_ function
  * Fourth, transpose the result using _t()_, since the result will be an $8 x 7$ object with rows corresponding with each possible predictor and columns corresponding with each model/adaptation
    * As a reminder, our desired object is a $7 x 8$ object, with model/adaptation in each row and possible predictor frequency in each column
  * Fifth, make the result a dataframe
  * Sixth, make the result a data.table, _again_
    * I can't quite explain why we need to conduct the steps above in the exact order (and repetition) in the pipes above. There's a sequence of transformations on the unstored/transitory object that impact what the result will be that mean _t()_ has to go in the middle, and _data.frame()_ at the end. But then we have to reset the object as a data.table after _data.frame()_.
  * Seventh, create a column of $100\%$-selection variables by mapping the _all()_ function over the rows of our intermediate data.table
  * Eighth, create a column of $0\%$-selection variables by mapping the _none()_ function over the rows of our intermediate data.table
  * Ninth, make a column indicating the adaptation for each row

Because I'm currently learning about keys, the last operation of the pipe sets a key for the _"Frequences"_ data.table based on the method/adaptation. (If you don't know what this means, don't worry too much about it. It won't generally come up for the purposes of demonstrating my combined model. Right now its only practical significance is alphabetically sorting _"Frequencies"_ by method/adaptation).

Let's take a quick look at the structure of our coefficient frequencies object. Note that if we had not set the key, the final line would not be present. This tells us that the data.table is sorted by the character values in _"method"_.

```{r Frequencies str}
str(models_all[["Frequencies"]][["By_Adaptation"]])
```

### Coefficient frequency processing: By Potential Predictor

Now we're going to do the same thing, except by predictor instead of adaptation. This is actually pretty easily adapted from the script used to generate a data.table by adaptation. We can just eliminate the transpose ( _t()_ ) step of the pipe. As a result, we can also get rid of the initial _setDT()_ step, as it is now redundant. The only other meaningful changes:
  * change "all"/"none" columns to map the _adapts_100()_ & _adapts_0()_ functions instead of the previous _all_splits()_ & _no_splits()_ functions
  * Replace the "method" column with a "predictor" column indicating the potential predictor with which each data.table row corresponds

```{r run coef frequency function on ladlasso models}
models_all[["Frequencies"]][["By_Predictor"]] <- models_all[1:7] %>%
        map(coefs_freq) %>%
        data.frame %>%
        setDT %>%
        .[ , "all" := apply(. , 1 , adapts_100)] %>%
        .[ , "none" := apply(. , 1 , adapts_0)] %>%
        .[ , "predictor" := names(models_all[["Frequencies"]][["By_Adaptation"]])[1:8]] %>%
        setkey(. , predictor) 
```

Let's take a look at the new data.table using _str()_.

```{r Frequencies str}
str(models_all[["Frequencies"]][["By_Predictor"]])
```

### Another Way

Below is another pair of commands which will generate our selection frequencies table, with two major differences. Since the version below involves three separate commands, rather than a single set of operations in one command, we'll use the first version outlined above. Note also that the example below only generates the frequency table by adaptation, but can be edited slightly to generate the table by predictor.
  * First, we set the object location/pointer at the end of the pipe instead of the beginning
  * Second, because of that ordering, we can't define the new column within _"Frequencies"_ in the same pipes
    * *Think about*: Why not?

```{r another way , eval = F}
models_all %>%
        map(coefs_freq) %>%
        setDT %>%
        t %>%
        data.frame %>%
        setDT -> models_all[["Frequencies"]][["By_Adaptation"]]

models_all[["Frequencies"]][["By_Adaptation"]][ , "method" := names(models_all)[-8]]

setkey(models_all[["Frequencies"]][["By_Adaptation"]] , method)

```

## VISUALIZE
