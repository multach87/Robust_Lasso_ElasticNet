---
title: 'Combined-Model Testing'
author: "Matt Multach"
date: "`r Sys.Date()`"
output: html_document
#output: 
#       xaringan::infinite_moon_reader:
#              lib_dir: lib
#              highlightStyle: github
#              highlightLines: true
#              countIncrementalSlides: false
#              beforeInit: "macros.js"
#              css: [default, tamu, tamu-fonts]
---

## Introduction

This file is used to test out the combined-model approach proposed in ADD DISSERTATION PROQUEST CITATION WHEN PUBLISHED.

## Preamble

```{r setup, message = FALSE , warning = FALSE , include=FALSE}
knitr::opts_chunk$set(echo = TRUE , eval = TRUE)
```

First, let's load the necessary packages. I've also set this code chunk to not print warnings, as they are largely not helpful for the current example.

```{r libraries , warning = FALSE}
# This chunk loads the packages used in this workbook
library(xaringan)   # Allows active preview of report in RStudio
library(mvtnorm)    # Generates multivariate-normal data
library(magrittr)   # Used for piping
library(purrr)      # Used for mapping functions efficiently
library(data.table) # For more streamlined data structures
library(glmnet)     # For general lasso/elastic net functionality
library(hqreg)      # For LAD/Huber-loss regularization with SNCD algorithm
library(rlang)      # For parse_expr() to parse data name in k-fold subsetting functions
library(msaenet)    # For the multi-step adaptive elastic net
```


Let's also load our test data, which will just be our matricized singular applied dataset.

```{r load synthetic applied dataset}
demo.data <- readRDS("/Users/Matt/Desktop/GitHub_Dissertation_Scripts/Robust_Lasso_ElasticNet/Datasets/TestingApplied_Xmtx.RData")
```

Let's also quickly combine the X and Y list elements into a single object for convenience with the subsetting function. We will put Y into the first column since it corresponds with a left-to-right reading of a regression model.

```{r combine X and Y into single object}
demo.data2 <- setDT(data.frame(cbind(demo.data[["Y"]] , demo.data[["X"]]
                                     )
                               )
                    )
```

## Loading custom functions

### Custom k-fold subsetting function

The chunk below takes a dataset and creates an ordinal variable with *k* levels, corresponding with each of the *k* test sets. The function takes as its arguments:

 * **data** : a dataset to be split into *k* groups
 * ***k*** : the number of folds for cross-validation
   * Defaults to 5 folds
   * Corresponds with the percentage of data to be used as a holdout/testing set in the following manner:
   $$
   \begin{equation}
        n_{test} = \text{
                $n_{full}$ $/$ $k$
        }
   \end{equation}
   $$
 * **seed** : a seed for fixing random processes
   * For this function, this is primarily associated with random and unordered assignment of the values of *k* across the full dataset
 * **list** : this argument tells the function if you want the resulting *k*-fold indexed data to be put into list format, with each list element corresponding with one fold *k*
   * This function defaults to *FALSE*, meaning the resulting data will be in matrix/tabular format and will contain a separate column for the index *k*
   * If set to *TRUE*, the resulting list will *not* contain the index variable for *k*, as the list elements themselves correspond with *k*
   * If set to *"traintest"* (in quotes!), the resulting list will instead contain two elements:
     * A training set containing data from *k* - 1 folds/subsets
     * A holdout/testing set containing data from the *k* fold/subset

```{r kfold subsetting function}
# k-fold subsetting function
kfold_subsetter <- function(data , y_col = 1 , 
                            x_cols = c(2:ncol(data)) , 
                            subset_col = (ncol(data) + 1) , 
                            k = 5 , seed = 7 , list = FALSE) {
        # check for string object in data argument
        if(is.character(data)) {
                data <- eval(parse_expr(data))
        }
        
        # check for data.table and setDT() if not a data.table
        if(!(TRUE %in% (class(data) == "data.table"))) {
                data <- setDT(data.frame(data))
        }
        
        # check for 0 < k <= n/2
        if((k <= 0) | (k > (nrow(data) / 2) ) ) {
                stop("ERROR: number of folds 'k' must be greater than 0 and less than sample size divided by 2")
        }
        
        # determine number of subsets which contain an extra element
        # # if n is not evenly divisible by k
        # # # note that this value will be 0 if n/k is evenly divisible
        nsams.large <- nrow(data) %% k
        
        # determine number of smaller subset if n 
        # # is not evenly divisible by k
        # # # note that this will be the total number of samples if n/k is evenly divisible
        nsams.small <- k - nsams.large
        
        # determine sample size of larger subsets if applicable
        samsize.large <- ceiling(nrow(data) / k) * (nsams.large != 0)
        
        # determine sample size of smaller/all subsets
        samsize.small <- floor(nrow(data) / k)
                
        # create indicator for each subset
        subset.indicator <- c(rep( (1 : k) , 
                                   floor(nrow(data) / k)
                                  ) ,
                              rep( (1 : (nsams.large) ) , 
                                   (1 * (nsams.large != 0) ) 
                                  )
                              )
                
        # fix random assignment process
        if(seed) {
                set.seed(seed)
        }
        
        # combine subset indicator with original data  
        newdata <- cbind(data , 
                         subset = sample(subset.indicator)
                         )
                
        newdata <- setDT(data.frame(newdata))
        
        # create k-split list if desired
        if(list == TRUE) {
                newdata <- return(split(newdata , 
                                        newdata[ , "subset"])
                                  )
        } else if(list == "traintest") {
                newdata <- return(list(
                    X = subset(newdata[ , c(x_cols, subset_col) , 
                                            with = F] , subset < k) %>%
                        .[ , c(1:8) , with = F] %>%
                        as.matrix() , 
                    Y = subset(newdata[ , c(y_col , subset_col) , 
                                        with = F] , subset < k) %>%
                        .[ , 1 , with = F] %>%
                        as.matrix() , 
                    X_Test = subset(newdata[ , c(x_cols, subset_col) , 
                                            with = F] , subset %in% k) %>%
                        .[ , c(1:8) , with = F] %>%
                        as.matrix() , 
                    Y_Test = subset(newdata[ , c(y_col , subset_col) , 
                                        with = F] , subset %in% k) %>%
                        .[ , 1 , with = F] %>%
                        as.matrix() , 
                    Seed = seed , 
                    Subsets = newdata[ , "subset"]
                        #Train = subset(newdata , subset < k) , 
                        #Test = subset(newdata , subset %in% k) , 
                        #Seed = seed
                        )
                        )
        } else {
                newdata <- return(newdata)
        }
}



```

### Multi-Subsetting Wrapper

The following function serves as a wrapper around the original subsetting function which incorporates additional arguments. These arguments extend the original function to conduct multiple training-testing splits and store the resulting datasets in a singular overarching list.

The wrapper's arguments are as follows. The ellipses are arguments for the original subsetting function which are handled internally within the wrapper.

 * **data** : a dataset to be split into training-testing sets
 * **seed_multi** : a seed for generating the vector of seeds for each individual training-testing *k*-fold split
 * **num_splits** : the number of training-testing splits to be created
    * Defaults to 100
 * **test_percent** : the percentage (as a decimal) of the full data to be reserved for the holdout/testing set
    * Corresponds with the number of folds *k* for *k*-fold subsetting in the following manner:
   $$
   \begin{equation}
        test\_percent = \text{
                $1$ $/$ $k$
        }
   \end{equation}
   $$
    * Defaults to 20$\%$, which corresponds with 5-fold subsetting

Note that this wrapper defaults to setting "list" argument in *kfold_subsetter()* to "traintest." This decision was implemented to reduce issues with formatting an applied dataset for the final combined-model function. The subsetting function is relatively more flexible in terms of data inputs than the full combined-model function, therefore I wanted to streamline data formatting from that point forward when using the combined model in full.

```{r multi-kfold wrapper}
kfold_multi <- function(data , ... , seed_multi = 713 , 
                        num_splits = 100 , test_percent = .2) {
        # set the seed for generating individual kfold seeds
        set.seed(seed_multi)
        
        # turn data object name into character string for subsequent use
        data.name <- deparse(substitute(data))
        
        # generate random numbers equal to num_splits so that each
        # # individual run of the subsetter has a unique seed
        seeds <- sample(c(1:100000) , size = num_splits , replace = FALSE)
        
        # set value of k corresponding with test_percent
        if((test_percent >= 1) | (test_percent <= 0)) {
                stop("ERROR: 'test_percent' must be greater than and less than 1")
        }
        
        # set k for individual splits
        k <- 1 / test_percent
        
        # initialize and data.table of kfold arguments
        split_repped.dt <- setDT(as.data.frame(matrix(ncol = 1 , 
                                              nrow = num_splits)))
        
        # fill data.table with arguments for kfold function
        split_repped.dt[ , ':=' (data = data.name , 
                         k = k , 
                         seed = seeds , 
                         list = "traintest"
                         )
                 ]
        
        # remove blank column from initializing
        split_repped.dt <- split_repped.dt[ , !1 , with = F]
        
        # run subsetter for the desired number of splits
        full.data <- split_repped.dt %>%
                pmap(kfold_subsetter)
        
        # store single object of all training/testing splits
        return(full.data)
}
```

### Model-Application Function

Finally, let's load the function used to generate our the models from our lasso and elastic net adaptations.

```{r hqreg model application function , echo = T}

# adaptive LAD lasso application function
hqmsa.sim.fnct <- function(data.list , 
                           method = c("msaenet" , 
                                      "quantile" , "LAD" , 
                                      "huber") , 
                           tau = 0.5 , 
                           gamma = 1.345 , 
                           alpha = 0.5 , 
                           nsteps = 10L , 
                           print.time = TRUE) {
       # Store training X and Y to temporary objects
       X_train <- data.list[["X"]]
       Y_train <- data.list[["Y"]]
       
       if(method == "LAD") {
               method <- "quantile"
       }
       
       # If applicable, store holdout/testing X and Y
       if(!is.null(data.list[["X_test"]]) & 
          !is.null(data.list[["Y_test"]])) {
               X_test <- data.list[["X_test"]]
               Y_test <- data.list[["Y_test"]]
       } else {
               X_test <- NULL
               Y_test <- NULL
       }
        
       # lambdas to try for regularization
       lambda.try <- seq(log(1400) , log(0.01) , length.out = 100)
       lambda.try <- exp(lambda.try)
       
       # set a timer start point
       start <- Sys.time()
       
       # cross-validated selection of adaptive lasso
       # # tuning hyperparameter nu/gamma
       
       # # select ridge coefs for weighting
       ridge.model <- cv.glmnet(x = X_train , y = Y_train , 
                                lambda = lambda.try , alpha = 0)
       lambda.ridge.opt <- ridge.model$lambda.min
       best.ridge.coefs <- predict(ridge.model , 
                                           type = "coefficients" ,
                                           s = lambda.ridge.opt)[-1]

       
       # # grid of nu/gamma values to try for cross-validation
       nu.try <- exp(seq(log(0.01) , log(10) , length.out = 100))
       
       # # initialize full list of LAD lasso results from each nu/gamma
       hqmsa.nu.cv.full <- list()
       
       # # initialize matrices of metrics and minimizing results
       hqmsa.nu.cv.lambda <- numeric()
       hqmsa.nu.cv.mse <- numeric()
       hqmsa.nu.cv.msesd <- numeric()
       hqmsa.nu.cv.coefs <- list()
       
       # # Loop over nu/gamma values for CV, 
       # # # storing minimizing lambda within each nu/gamma
       if(method == "msaenet") {
               for(i in 1:length(nu.try)) {
                       #single adaptive lasso run with ridge weighting and nu = 1
                       hqmsa.nu.cv.full[[i]] <- msaenet(x = X_train , 
                                                    y = Y_train , 
                                                    family = "gaussian" , 
                                                    init = "ridge" ,
                                                    alphas = 0.5 , 
                                                    tune = "cv" , 
                                                    nfolds = 5L , 
                                                    rule = "lambda.min" , 
                                                    nsteps = nsteps , 
                                                    tune.nsteps = "max" , 
                                                    scale = nu.try[i])
                       
                       hqmsa.nu.cv.lambda[i] <-
                               hqmsa.nu.cv.full[[i]]$best.lambdas[[nsteps + 1]]
                       
                       hqmsa.nu.cv.coefs[[i]] <- c(NA , coef(hqmsa.nu.cv.full[[i]]))
                               
                       
                       hqmsa.nu.cv.mse[i] <- min(hqmsa.nu.cv.full[[i]]$step.criterion[[nsteps + 1]])
                       }
       } else {
               for(i in 1:length(nu.try)) {
                       invisible(capture.output(
                               hqmsa.nu.cv.full[[i]] <- 
                                       cv.hqreg(X = X_train , 
                                                y = Y_train , 
                                                method = method , 
                                                tau = tau , 
                                                gamma = gamma , 
                                                lambda = lambda.try ,
                                                alpha = alpha , 
                                                preprocess =
                                                        "standardize" , 
                                                screen = "ASR" , 
                                                penalty.factor = 
                                                        1 / abs(best.ridge.coefs) ^ nu.try[i] , 
                                                FUN = "hqreg" , 
                                                type.measure = "mse"
                                                )
                                        )
                               )
                       hqmsa.nu.cv.mse[i] <-
                               min(hqmsa.nu.cv.full[[i]]$cve)
                       hqmsa.nu.cv.msesd[i] <-
                               hqmsa.nu.cv.full[[i]]$cvse[
                         which.min(hqmsa.nu.cv.full[[i]]$cve)
                                                         ]
                       hqmsa.nu.cv.lambda[i] <-
                               hqmsa.nu.cv.full[[i]]$lambda.min
                       hqmsa.nu.cv.coefs[[i]] <- 
                               hqmsa.nu.cv.full[[i]]$fit$beta[ , 
                         which.min(hqmsa.nu.cv.full[[i]]$cve)
                                                             ]
                       }
       }

       
       #specify minimizing nu value and resulting model info
       nu.opt <- nu.try[which.min(hqmsa.nu.cv.mse)]
       lambda.opt <- 
               hqmsa.nu.cv.lambda[
                       which.min(hqmsa.nu.cv.mse)
                       ]
       weights.opt <- 1 / abs(best.ridge.coefs) ^ nu.opt
       hqmsa.coefs <-
               hqmsa.nu.cv.coefs[[
                       which.min(hqmsa.nu.cv.mse)
                       ]]
       hqmsa.mse.min <- min(hqmsa.nu.cv.mse)
       if(!is.null(hqmsa.nu.cv.msesd[1])) {
               hqmsa.mse.min.se <- hqmsa.nu.cv.msesd[
                       which.min(hqmsa.nu.cv.mse)
                       ]               
       }

       hqmsa.model.min <- 
               hqmsa.nu.cv.full[
                       which.min(hqmsa.nu.cv.mse)
                       ]
       n.coefs <- sum(hqmsa.coefs[-1] != 0)
       
       # calculate metrics using holdout data, if applicable
       if(!is.null(X_test) & !is.null(Y_test)) {
               # store n
               n <- nrow(data.list[["X_test"]])
               
               # calculate predicted values
               y.pred <- data.list[["X_test"]] %*% hqmsa.coefs[-1]
               if(!is.na(hqmsa.coefs[1])) {
                       y.pred <- y.pred + hqmsa.coefs[1]
               }
   
               # calculate residual
               resid <- y.pred - Y_test
               
               # square the residuals
               resid.sq <- resid ^ 2
               
               # sum the square of residuals
               sum.resid.sq <- sum(resid.sq)
               
               #calculate root mse
               mse <- sum.resid.sq / n
               
               # set endpoint for timer
               end <- Sys.time()
               
               # temporarily store time of current model
               time <- abs(as.numeric(difftime(start , 
                                               end , 
                                               units = "secs"
                                               )
                                      )
                           )
               
               # print the total runtime of the current model
               if(print.time) {
                       cat("time = " , time , " ;;; ")
               }
               
               # put conditions, model info, and metrics into list
               return(list(full.model = hqmsa.model.min ,
                           model.info = list(lambda = lambda.opt , 
                                             coefs = hqmsa.coefs , 
                                             weights = weights.opt
                                             ) , 
                           metrics = list(n.coefs = n.coefs , 
                                          runtime = time , 
                                          mse = mse
                                          )
                           )
                      )
       } else {
               # set endpoint for timer
               end <- Sys.time()
               
               # temporarily store time of current model
               time <- abs(as.numeric(difftime(start , 
                                               end , 
                                               units = "secs"
                                               )
                                      )
                           )
               
               # print the total runtime of the current model
               if(print.time) {
                       cat("time = " , time , " ;;; ")
               }
               
               # put conditions, model info, and metrics into list
               return(list(full.model = hqmsa.model.min ,
                           model.info = list(lambda = lambda.opt , 
                                             coefs = hqmsa.coefs , 
                                             weights = weights.opt
                                             ) , 
                           metrics = list(n.coefs = n.coefs , 
                                          runtime = time
                                          )
                           )
                      )
       }
       
       

       

}
```

### Splitting the Data

Before we apply our models, let's create our data splits. We will use the multi-subsetting wrapper above to generate 100 random data splits. We will reserve 20$\%$ of the data for holdout/generating prediction error (aka, 5-fold subsets).

```{r splitting data}
demo.split <- list()

demo.split<- kfold_multi(demo.data2 , seed_multi = 999 , 
                         num_splits = 100 , test_percent = 0.2)
```

Here is a good place to stop and save the split data if you want store it for later use using the _saveRDS()_ function.

## Running our models

Now we can generate the models from our adaptations! I opted to generate the 100-model objects for each adaptation separately and then combined them. A little work with tools use throughout the different component demos could make a wrapper that generates all with one command and store them in a single objects. I will leave that as an exercise for anyone looking to practice function-writing. 

We are going to generate models from each of the following adaptations:

 * Adaptive LAD asso
 * Adaptive LAD elastic net, $\alpha = 0.5$
 * Adaptive Huber lasso, $M = 1.345$
 * Adaptive Huber elastic net, $M = 1.345, \alpha = 0.5$
 * Multi-step adaptive elastic net with $k = 3$ adaptive steps, $\alpha = 0.5$
 * Multi-step adaptive elastic net with $k = 5$ adaptive steps, $\alpha = 0.5$
 * Multi-step adaptive elastic net with $k = 10$ adaptive steps, $\alpha = 0.5$

For the purposes of testing, this demo only runs the models on 10 data splits each. The full demo of the combined-model approach will run each model across all 100 data splits.

### Adaptive LAD Lasso

```{r ladlasso}
#run across data subset
ladlasso.10 <- demo.split[1:10] %>%
        map(safely(hqmsa.sim.fnct) , method = "LAD" , alpha = 1)
```

### Adaptive LAD Elastic Net, Alpha = 0.5

```{r ladelnet5}
#run across data subset
ladelnet5.10 <- demo.split[1:10] %>%
        map(safely(hqmsa.sim.fnct) , method = "LAD" , alpha = 0.5)
```

### Adaptive Huber Lasso

```{r huberlasso}
#run across data subset
huberlasso.10 <- demo.split[1:10] %>%
        map(safely(hqmsa.sim.fnct) , method = "huber" , alpha = 1)
```

### Adaptive Huber Elastic Net, Alpha = 0.5

```{r ladelnet5}
#run across data subset
huberelnet5.10 <- demo.split[1:10] %>%
        map(safely(hqmsa.sim.fnct) , method = "huber" , alpha = 0.5)
```

### Multi-Step Adaptive Elastic Net, k = 3, Alpha = 0.5

```{r msaenet k3}
#run across data subset
msaelnetk3.10 <- demo.split[1:10] %>%
        map(safely(hqmsa.sim.fnct) , method = "msaenet" , nsteps = 3L)
```

### Multi-Step Adaptive Elastic Net, k = 5, Alpha = 0.5

```{r msaenet k5}
#run across data subset
msaelnetk5.10 <- demo.split[1:10] %>%
        map(safely(hqmsa.sim.fnct) , method = "msaenet" , nsteps = 5L)
```

### Multi-Step Adaptive Elastic Net, k = 10, Alpha = 0.5

```{r msaenet k10}
#run across data subset
msaelnetk10.10 <- demo.split[1:10] %>%
        map(safely(hqmsa.sim.fnct) , method = "msaenet" , nsteps = 10L)
```

### map(safely())

You may notice that in mapping each model across the first 10 data splits, we include an extra function _safely()_ within _map()_, in addition to the subsequent arguments to specify the adaptations of interest. If we look at the structure of one model from one adaptation

```{r str safely explain}
str(ladlasso.10[[1]] , max.level = 2)
```

we notice a list with 2 elements: 'result' and 'error', with 'result' containing 3 list elements. Using just _map()_ would have produced one of the following:
 * A list of 3 elements for each data split: _full.model_, _model.info_, and _metrics_
 * The mapping command would have failed due to an error

The _safely()_ function allows the mapping command to continue in the face of an error rather than stop completely. If an error had been produced by one of the models, the corresponding 'result' element would instead have been NULL. The 'error' element, on the other hand, would contain whatever error message was thrown while trying to map our model-application function.

Another note: in running _str()_ above, I specified an argument _max.level = 2_. _str()_ therefore only produced the structure to a depth of 2. If I had specified _max.level_, _str()_ would have given the structure for all objects down to the deepest level of object contained in _ladlasso.10_.

### Combine and save a temporary full model set

Just in case, I am going to combine all of the model files into one ugly dataset and save it.

```{r combine and save models , eval = F}
models_combine <- list(ladlasso = ladlasso.10 , ladelnet = ladelnet5.10 , 
                       huberlasso = huberlasso.10 , huberelnet = huberelnet5.10 , 
                       msak3 = msaelnetk3.10 , msak5 = msaelnetk5.10 , 
                       msak10 = msaelnetk10.10)

saveRDS(models_combine , "/Users/Matt/Desktop/GitHub_Dissertation_Scripts/Robust_Lasso_ElasticNet/models_combine10.RData")
```


## Processing the model results

There are a few ways we could process and organize the model results. There 2 main steps of processing that need to occur:
 * Move either the "result" or "error" list elements from each model up a level in the list
 * Put the model results from the different adaptations into a single combined object

### Get rid of result/error level of each model: Function

Fun fact: the _ifelse()_ results in an object with the same dimensions as the input object. Our input data will be the individual model list elements, corresponding with the result/error list that the model produced for each data split. Unfortunately, the results of each model are lists with 3 elements. Consequently, the code chunk below will only store the first of the 3 elements that we want stored.

```{r ifelse error result function , eval = F}
res_or_err <- function(data) {
    return(ifelse(is.null(data[["error"]] , data[["result"]] , data[["error"]])
                  )
           )
}
```

I didn't know this before working on this demo! As a result of this interesting behavior, the function which will be mapped onto each individual model will isntead utilize separate if-else statements.

```{r error or result function}
#eliminate result/error level
res_or_err <- function(data) {
        if(is.null(data[["error"]])) {
                temp <- data[["result"]]
        } else {
                temp <- data[["error"]]
        }
        
        return(temp)
}
```

### Get rid of result/error and combine

```{r no resulterror and combine models}
models_all <- list(ladlasso = ladlasso.10 %>%
                       map(res_or_err) , 
                   ladelnet = ladelnet5.10 %>%
                       map(res_or_err) , 
                   huberlasso = huberlasso.10 %>%
                       map(res_or_err) , 
                   huberelnet = huberelnet5.10 %>%
                       map(res_or_err) , 
                   msak3 = msaelnetk3.10 %>%
                       map(res_or_err) , 
                   msak5 = msaelnetk5.10 %>%
                       map(res_or_err) ,
                   msak10 = msaelnetk10.10 %>%
                       map(res_or_err))
```