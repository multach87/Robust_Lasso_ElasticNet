---
title: "Full Applied Walkthrough"
author: "Matt Multach"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    toc: true
    toc_depth: 4
    toc_float: true
    number_sections: false
bibliography: Component_Demos/Component_Refs.bib
---

## Preamble and Setup {.unlisted .unnumbered}

```{r setup, message = FALSE , warning = FALSE , include=FALSE}
knitr::opts_chunk$set(echo = TRUE , eval = TRUE)
```

#### A note on formatting

This document makes use of **bolding**, _italics_, and "quotations" to help distinguish different types of items being referenced.

 * **bolding** will be used when referencing a new term or concept for the first time. Subsequent references to each term/concept will _not_ be bolded.
 * _italics_ will be used primarily for emphasis.
 * 'single quotations' will be used to clarify specific arguments for a function, or specific parameters of a mathemtical/statistical formulation
 * Inline references to code (functions, objects, specific commands, etc.) will use **<span style = "color:blue">`` `r "code_chunk_formatting"` ``</span>** in **<span style = "color:blue">`` `r "bolded blue font"` ``</span>**
 * Inline references to packages will similarly use *<span style = "color:red">`` `r "code_chunk_formatting"` ``</span>*, except in *<span style = "color:red">`` `r "italicized red font"` ``</span>*
 * References to other documents in this repository will use <span style = "color:green">`` `r "code_chunk_formatting"` ``</span> in <span style = "color:green">`` `r "un-italicized, un-bolded, green font"` ``</span>
 
Re: spacing and line breaks - I'm pretty heterogeneous in my application of line breaks and spacing, in a way that is idiosyncratic to my own code practice. The most important aspects of my spacing and line breaks are detailed below.

I generally put spaces between code inputs I consider "sufficiently distinct". This improves readability generally, but I find it particularly helpful for debugging. Note, however, that spaces are generally trivial in between distinct code inputs in R, although this is not universally the case. Multi-character code inputs, such as the pointer **<span style = "color:blue">`` `r "<-"` ``</span>** and most logical operators, _cannot_ include spaces in between components of the code input. Note also that whitespace *is* meaningful in other programming languages, and so this convention should be considered with caution in your own practice.

Generally, I use line breaks to:

 * Break up separate arguments for a single command/function or chain of operations
 * To clearly distinguish between different closing parentheses, brackets, squigglies, etc., since RStudio will automatically tab each closing piece to match its opener.

#### Packages

First, let's load the necessary packages. Links to more information about each packages and helpful guides (where applicable) can be found in <span style = "color:green">`` `r "00B_Package_Descr_Refs"` ``</span>. Appropriate references for each package can be found in the "References" section at the end of this document.

```{r libraries , warning = FALSE , message = FALSE}
# This chunk loads the packages used in this workbook
library(xaringan)   # Allows active preview of report in RStudio
library(mvtnorm)    # Generates multivariate-normal data
library(magrittr)   # Used for piping
library(purrr)      # Used for mapping functions efficiently
library(data.table) # For more streamlined data structures
library(glmnet)     # For general lasso/elastic net functionality
library(hqreg)      # For LAD/Huber-loss regularization with SNCD algorithm
library(rlang)      # For parse_expr() to parse data name in k-fold subsetting functions
library(msaenet)      # For the multi-step adaptive elastic net
library(gridExtra)    # For displaying grids of objects
library(grid)         # For creating a title for multiplot grid
library(ggplot2)      # For generating and manipulation graphical objects
```

---
nocite: |
  @magrittr , @purrr , @xaringan , @data.table , @glmnet , @hqreg , @rlang , @msaenet , 
  @gridExtra , @grid , @ggplot2 , @magrittr
---

Note that if you do not have any of the listed packages, you should install them using **<span style = "color:blue">`` `r "install.packages(\"pkg-name-in-quotes-here\")"` ``</span>**.


## Introduction

This document combines components described in the various Component_Demos files to implement a combination of lasso and elastic net adaptations for improving variable selection capabilities of these adaptations.

The goal of this walkthrough is to demonstrate the combined approach I propose in the applied chapter of my dissertation, @MattDiss. Over the course of my dissertation, I showed that with non-collinear data, no one lasso or elastic net adaptation dominates others across outlier- and skew-/kurtosis-related scenarios. Some adaptations, like the **multi-step adaptive elastic net**, @XiaoXu2015, perform incredibly well at eliminating variables which were not true predictors of the outcome, even in the presence of extreme outlier contamination. This strong elimination performance unfortunately coincided with a greater tendency to remove true predictors from the model. 

Other methods also performed well in the opposite direction, including the adaptive LAD lasso, @Wangetal2007; the adaptive LAD elastic net, currently un-proposed but possible thanks to @YiHuang2017 and their *<span style = "color:red">`` `r "hqreg"` ``</span>* package, @hqreg; the adaptive Huber lasso, @RossetZhu2007 and  @LambertLacroixZwald2011; and the adaptive Huber elastic net, @YiHuang2017. These adaptations demonstrated consistently competitive performance in proper inclusion of true predictors and exclusion of non-predictors form a model. However, these adaptations typically did not show superior performance across scenarios relative to other adaptations.

Given these various results, I ultimately proposed a combined approach to implementing the more useful adaptations in a way that leverages their respective strengths. By taking a holistic view of the results across these methods, subjective interpretation can permit us better insight into true predictors of an unknown process.

Much room exists to develop objective and empirical metrics for proper application of such an approach. **Figure 1** below presents the ideal form of this combined approach, given empirically-derived metrics for variable inclusion and exclusion. **Figure 1** also suggests relevant research questions for guiding the development of said objective selection metrics. These questions are indicated below the main diagram in orange boxes.

An important note regarding one step of this process and related components of these demonstrations. While conducting my dissertation, I decided to utilize cross-validation splits as an ad-hoc resampling procedure given that out-of-sample prediction error was already a part of my metrics of interest in that research. However, an important question to add to this line of research would be:

 * What resampling procedure provides the most utility for improving variable selection?
    * Furthermore: How many resamples should be taken? For Example: If using a Bootstrap method, how many bootstrap samples provides the most robust variable selection characteristics?
    
In the months since defending my dissertation, I completely forgot how I created these wonderful charts. Once I am able to find the website, I will update **Figure 1** to include the additional step and research question.

![Figure 1: Empirically-Driven Approach for Combined Lasso and Elastic Net Model Application for Variable Selection](Combined_Method.png)

**Figure 2**, meanwhile, presents a version of this approach which utilizes subjective interpretation in lieu of objective and empirically-derived metrics and until such metrics are developed. 

![Figure 2: Subjective Approach for Combined Lasso and Elastic Net Model Application for Variable Selection](Combined_Qual.png)

The current walkthrough presents a slimmed-down version of the subjective approach. I will not spend too much time on any of the given functions or procedures conducted in the current demontration. I focus primarily on the application and interpretation of combined model approach. Consequently, I generally set code chunks in the preliminary setup portion to **<span style = "color:blue">`` `r "echo = F"` ``</span>** so that they are not included in the compiled html file, but will be in the .Rmd file. I will also include references to the appropriate component demonstrations where appropriate; please see the files in the <span style = "color:green">`` `r "Component Demonstrations"` ``</span> section for details on any of the individual components or functions which make up the original




## Data Generation

### Random Betas function and Loading **<span style = "color:blue">`` `r "data_gen()"` ``</span>**

To help demonstrate the combined approach in its entirety, I'm going to generate a dataset using an adapted version of my **<span style = "color:blue">`` `r "data_gen()"` ``</span>** function. This time, however, I'm going to generate a dataset with random characteristics which we will not look at until after we've interpreted the results. That way we can mimic a true applied dataset where we do *not* know the true underlying distribution or data-generating mechanism.

To facilitate this, I'm going to make a simple function for randomly generating beta coefficients. Note that the process essentially samples coefficients from the vector **<span style = "color:blue">`` `r "c(0.5 , 1 , 1.5 , 2 , 0 , 0 , 0 , 0)"` ``</span>**.

```{r random betas}
betas.rand <- function(x) {
        return(sample(x = c(0.5 , 1 , 1.5 , 2 , 0) , 
                      size = length(x) , replace = TRUE ,
                      prob = c(0.125 , 0.125 , 0.125 , 0.125 , 0.5)
                      )
               )
}
```

The original data-generating function can be found in <span style = "color:green">`` `r "01_Data_Generation"` ``</span>.

```{r data-generating function , echo = F}
data_gen <- function(n , p , eta_x , eta_y , g , h , 
                     seed , tracker) {      
  
  # create a data.table of the conditions of the current dataset
  conditions <- setDT(data.frame(n = n , p = p , 
                                 eta_x = eta_x , eta_y = eta_y , 
                                 g = g , h = h , tracker = tracker , 
                                 seed = seed)
                      )

  # create a p-column data.table containing all 0's
  betas <- setDT(as.data.frame(matrix(0 , nrow = 1 , ncol = p)
                               )
                 )
  # sample coefficients from (0.5 , 1 , 1.5 , 2 , 0 , 0 , 0 , 0)
  betas <- betas[ , lapply(.SD , betas.rand)] 
       
  # set seed for random process
  seed <- seed                       
       
  # generate covariance matrix
  covar.X <- matrix(rep(0 , p ^ 2) , ncol = p)  
  # # put 1's along diagonal of covariance matrix
  diag(covar.X) <- 1 
       
  # generate uncontam. X values
  X.UC <- rmvnorm(floor((1 - eta_x) * n
                        ) , 
                  mean = rep(0 , p) , 
                  sigma = covar.X
                  )
       
  # generate contaminated X/predictor values or 
  # # g-and-h-based X/predictor values
  if(((g == 0) & (h == 0) ) ){
        # if there is predictor contamination, generate the
        # # contaminated values
        if(eta_x > 0) {                             
                X.C <- rmvnorm(ceiling(eta_x * n) , 
                               mean <- rep(10 , p) , 
                               sigma = covar.X)
                X <- rbind(X.UC , X.C)
        # otherwise, set the uncontaminated values to the
        # # full X dataset
        } else {
                X.C <- 0
                X <- X.UC
        }
              
        #generate uncontom. residuals
        err.UC <- rnorm(floor((1 - eta_y) * n) , 
                        mean = 0 , sd = 1)
        if(eta_y > 0) {  
                # if there is response contamination, generate the
                # # contaminated values
                err.C <- rnorm(ceiling(eta_y * n) , mean = 2 , sd = 5)
                err <- c(err.UC , err.C)
              } else {
                # otherwise, set the uncontaminated values to the
                # # final residuals
                err.c <- 0
                err <- err.UC
              }
       } else if(((g != 0) | (h != 0))) {
         # generates X/predictor values from g-and-h distribution
         # # with no outlier contamination
         X <- X.UC
         err <- ghdist(n = n , g = g , h = h)
       }
  
  #generate Y values from X matrix, 
  # # coefficients vector, and residuals vector
  Y <- X %*% t(as.matrix(betas) ) + err                                    
  
  # Create list of separate components of generated data   
  combine <- list(conditions = conditions ,
                  tracker = tracker , 
                  seed = seed , 
                  betas = betas , 
                  X = setDT(data.frame(X)) , 
                  Y = setDT(data.frame(Y)) , 
                  err = err
                  )        
  
  #save combined list of all generated data
  return(combine)
}
```

### Generating the Data

I'm going to generate a dataset with a sample size of 100, 200 potential predictors, a random amount of outlier contamination in the predictors selected from $0\%$, $10\%$, and $20\%$, and a random amount of outlier contamination in the response variable selected from $0\%$, $10\%$, and $20\%$. No skew or kurtosis will be intentionally induced in the data.

#### UPDATE 11/18/2021-1

So it seems that with this n and p, null models (aka models with 0 coefficients) were generally being produced. It's unclear why this was happening, as even the lasso models should be at least producing some coefficients. These models have also worked in prior real-data applications with p > n when I was working on my dissertation.

Probably what's going on is that the idiosyncracies of the generated data give the models trouble. Turns out, this is *not* a real dataset.

In the meantime, I'm going to set the data to be generated with sample size of 100 and 50 potential predictors.

#### UPDATE 11/18/2021-2

Matt did another dumb when he originally wrote up the k-fold subsetting function. I made that function such that the new X objects would specifically have 8 columns - corresponding with the 8 predictors in my first first super original dataset I tested on in the early days of this repository. **<span style = "color:blue">`` `r "kfold_subsetter()"` ``</span>**, as well as the corresponding functions in various component demos, have all but updated to reflect this change. 

All that said, I am still going to reduce the number of potential predictors down to 50. Dealing with dimensionality adds a huge computational burden to the model estimation process, one which makes rendering an appropriate markdown file impractical on my current hardware. When finances permit, I plan on generating those models with an AWS EC2 instance and re-compiling this walkthrough, except setting certain model-estimating code chunks to **<span style = "color:blue">`` `r "eval = F"` ``</span>** and including hidden chunks that load the AWS-generating data.




```{r generate single dataset}
wt.data <- data_gen(n = 100 , p = 50 , 
                          eta_x = sample(x = c(0 , 0.1 , 0.2) , size = 1) , 
                          eta_y = sample(x = c(0 , 0.1 , 0.2) , size = 1) , 
                          g = 0 , h = 0 , 
                          seed = rnorm(1) , tracker = 1
                          )
```

## CV-Splitting data

Now we need to conduct cross-validation splits of the data. The custom k-fold subsetting functions I created and demonstrated in <span style = "color:green">`` `r "02A_KFold_Subsetter"` ``</span> and <span style = "color:green">`` `r "02B_Kfold_Multi"` ``</span> are hidden from the html file. The code for those functions can be found in the .Rmd, and further details about those functions can be found in the corresponding <span style = "color:green">`` `r "Component Demonstrations"` ``</span>.

```{r kfold subsetting function , echo = F}
# k-fold subsetting function
kfold_subsetter <- function(data , y_col = 1 , 
                            x_cols = c(2:ncol(data)) , 
                            subset_col = (ncol(data) + 1) , 
                            k = 5 , seed = 7 , list = FALSE) {
        # check for string object in data argument
        if(is.character(data)) {
                data <- eval(parse_expr(data))
        }
        
        # check for data.table and setDT() if not a data.table
        if(!(TRUE %in% (class(data) == "data.table"))) {
                data <- setDT(data.frame(data))
        }
        
        # check for 0 < k <= n/2
        if((k <= 0) | (k > (nrow(data) / 2) ) ) {
                stop("ERROR: number of folds 'k' must be greater than 0 and less than or equal to half of the sample size")
        }
        
        # determine number of subsets which contain an extra observation
        # # if n is not evenly divisible by k
        # # # note that this value will be 0 if n/k is evenly divisible
        nsams.large <- nrow(data) %% k
        
        # determine number of smaller subset if n 
        # # is not evenly divisible by k
        # # # note that this will be the total number of samples if 
        # # # # n/k is evenly divisible
        nsams.small <- k - nsams.large
        
        # determine sample size of larger subsets if applicable
        samsize.large <- ceiling(nrow(data) / k) * (nsams.large != 0)
        
        # determine sample size of smaller/all subsets
        samsize.small <- floor(nrow(data) / k)
                
        # create indicator for each subset
        subset.indicator <- c(rep( (1 : k) , 
                                   floor(nrow(data) / k)
                                  ) ,
                              rep( (1 : (nsams.large) ) , 
                                   (1 * (nsams.large != 0) ) 
                                  )
                              )
                
        # fix random assignment process
        if(seed) {
                set.seed(seed)
        }
        
        # combine subset indicator with original data  
        newdata <- cbind(data , 
                         subset = sample(subset.indicator)
                         )
        
        # create k-split list if desired
        if(list == TRUE) {
                newdata <- return(split(newdata , 
                                        newdata[ , "subset"])
                                  )
        } else if(list == "traintest") {
                newdata <- return(list(
                    X = subset(newdata[ , c(x_cols, subset_col) , 
                                            with = F] , subset < k) %>%
                        .[ , c(1:length(x_cols)) , with = F] , #%>% 
                        #as.matrix() , 
                    Y = subset(newdata[ , c(y_col , subset_col) , 
                                        with = F] , subset < k) %>%
                        .[ , 1 , with = F] , #%>%
                        #as.matrix() , 
                    X_Test = subset(newdata[ , c(x_cols, subset_col) , 
                                            with = F] , subset %in% k) %>%
                        .[ , c(1:length(x_cols)) , with = F] , # %>%
                        #as.matrix() , 
                    Y_Test = subset(newdata[ , c(y_col , subset_col) , 
                                        with = F] , subset %in% k) %>%
                        .[ , 1 , with = F] , # %>%
                        #as.matrix() , 
                    Seed = seed , 
                    Subsets = newdata[ , "subset"]
                    )
                    )
        } else {
                newdata <- return(newdata)
        }
}



```

```{r multi-kfold wrapper , echo = F}
kfold_multi <- function(data , ... , seed_multi = 713 , 
                        num_splits = 100 , test_percent = .2) {
        # set the seed for generating individual kfold seeds
        set.seed(seed_multi)
        
        # turn data object name into character string for subsequent use
        data.name <- deparse(substitute(data))
        
        # generate random numbers equal to num_splits so that each
        # # individual run of the subsetter has a unique seed
        seeds <- sample(c(1:100000) , size = num_splits , replace = FALSE)
        
        # set value of k corresponding with test_percent
        if((test_percent >= 1) | (test_percent <= 0)) {
                stop("ERROR: 'test_percent' must be greater than and less than 1")
        }
        
        # set k for individual splits
        k <- 1 / test_percent
        
        # initialize and data.table of kfold arguments
        split_repped.dt <- setDT(as.data.frame(matrix(ncol = 1 , 
                                              nrow = num_splits)))
        
        # fill data.table with arguments for kfold function
        split_repped.dt[ , ':=' (data = data.name , 
                         k = k , 
                         seed = seeds , 
                         list = "traintest"
                         )
                 ]
        
        # remove blank column from initializing
        split_repped.dt <- split_repped.dt[ , !1 , with = F]
        
        # run subsetter for the desired number of splits
        full.data <- split_repped.dt %>%
                pmap(kfold_subsetter)
        
        # store single object of all training/testing splits
        return(full.data)
}
```

### Create cv-split data

I'm going to conduct 100 splits, similarly to in the component demos and in my dissertation research. First I need to extract the X's and Y's into a single data object to be split.

```{r extract X and Y for cv splitting}
wt.data2 <- setDT(data.frame(cbind(wt.data[["Y"]] , wt.data[["X"]]
                                     )
                               )
                    )
```

```{r create cv-split data 100 splits}
wtcv.100 <- kfold_multi(data = wt.data2 , num_splits = 100)
```


### Save the Date-a

Let's save the original dataset, including the conditions which generated it, as well as our cv-split data.

```{r save walkthrough data}
saveRDS(wt.data , "/Users/Matt/Desktop/GitHub_Dissertation_Scripts/Robust_Lasso_ElasticNet/Datasets/walkthrough_data.RData")
```

```{r save 100 cv split walkthrough data}
saveRDS(wtcv.100 , "/Users/Matt/Desktop/GitHub_Dissertation_Scripts/Robust_Lasso_ElasticNet/Datasets/wtdata_cvsplit.RData")
```

## Generating Lasso and Elastic Net Models

Let's generate the models for each of our lasso and elastic net adaptations and each data split. As previously, the procedural code chunks are hidden from the compiled html walkthrough; the process and custom functions used to generate the full span of adaptation models can be found primarily in <span style = "color:green">`` `r "03D_Combined_Model_Generation"` ``</span> ([Demo 3D](https://multach87.github.io/Robust_Lasso_ElasticNet/Component_Demos/03D_Combined_Model_Generation.html)). Additional demonstrations of the steps leading up to the full model generation can be found in <span style = "color:green">`` `r "03A_AdaLAD_SingleDemo"` ``</span> ([Demo 3A](https://multach87.github.io/Robust_Lasso_ElasticNet/Component_Demos/03A_AdaLAD_SingleDemo.html)), <span style = "color:green">`` `r "03B_hqreg_generalized"` ``</span> ([Demo 3B](https://multach87.github.io/Robust_Lasso_ElasticNet/Component_Demos/03B_hqreg_generalized.html)), and <span style = "color:green">`` `r "03C_hqreg_msaenet_generalized"` ``</span> ([Demo 3C](https://multach87.github.io/Robust_Lasso_ElasticNet/Component_Demos/03C_hqreg_msaenet_generalized.html)).

*However*, because the actual application of the model function and the subsequent processing of the model results would be a part of the actual use of my combined approach in a real setting, I *will* be including those code chunks in the current walkthrough.

```{r hqreg model application function , echo = T}

# adaptive LAD lasso application function
hqmsa.sim.fnct <- function(data.list , 
                           method = c("msaenet" , 
                                      "quantile" , "LAD" , 
                                      "huber") , 
                           tau = 0.5 , 
                           gamma = 1.345 , 
                           alpha = 0.5 , 
                           nsteps = 5L , 
                           print.time = TRUE) {
       # Store training X and Y to temporary objects
       X_train <- as.matrix(data.list[["X"]])
       Y_train <- as.matrix(data.list[["Y"]])
       
       if(method == "LAD") {
               method <- "quantile"
       }
       
       # If applicable, store holdout/testing X and Y
       if(!is.null(data.list[["X_test"]]) & 
          !is.null(data.list[["Y_test"]])) {
               X_test <- as.matrix(data.list[["X_test"]])
               Y_test <- as.matrix(data.list[["Y_test"]])
       } else {
               X_test <- NULL
               Y_test <- NULL
       }
        
       # lambdas to try for regularization
       lambda.try <- seq(log(1400) , log(0.01) , length.out = 100)
       lambda.try <- exp(lambda.try)
       
       # set a timer start point
       start <- Sys.time()
       
       # cross-validated selection of adaptive lasso
       # # tuning hyperparameter nu/gamma
       
       # # select ridge coefs for weighting
       ridge.model <- cv.glmnet(x = X_train , y = Y_train , 
                                lambda = lambda.try , alpha = 0)
       lambda.ridge.opt <- ridge.model$lambda.min
       best.ridge.coefs <- predict(ridge.model , 
                                           type = "coefficients" ,
                                           s = lambda.ridge.opt)[-1]

       
       # # grid of nu/gamma values to try for cross-validation
       nu.try <- exp(seq(log(0.01) , log(10) , length.out = 100))
       
       # # initialize full list of LAD lasso results from each nu/gamma
       hqmsa.nu.cv.full <- list()
       
       # # initialize matrices of metrics and minimizing results
       hqmsa.nu.cv.lambda <- numeric()
       hqmsa.nu.cv.mse <- numeric()
       hqmsa.nu.cv.msesd <- numeric()
       hqmsa.nu.cv.coefs <- list()
       
       # # Loop over nu/gamma values for CV, 
       # # # storing minimizing lambda within each nu/gamma
       if(method == "msaenet") {
               for(i in 1:length(nu.try)) {
                       #single adaptive lasso run with ridge weighting and nu = 1
                       hqmsa.nu.cv.full[[i]] <- msaenet(x = X_train , 
                                                    y = Y_train , 
                                                    family = "gaussian" , 
                                                    init = "ridge" ,
                                                    alphas = 0.5 , 
                                                    tune = "cv" , 
                                                    nfolds = 5L , 
                                                    rule = "lambda.min" , 
                                                    nsteps = nsteps , 
                                                    tune.nsteps = "max" , 
                                                    scale = nu.try[i])
                       
                       hqmsa.nu.cv.lambda[i] <-
                               hqmsa.nu.cv.full[[i]]$best.lambdas[[nsteps + 1]]
                       
                       hqmsa.nu.cv.coefs[[i]] <- c(NA , coef(hqmsa.nu.cv.full[[i]]))
                               
                       
                       hqmsa.nu.cv.mse[i] <- min(hqmsa.nu.cv.full[[i]]$step.criterion[[nsteps + 1]])
                       }
       } else {
               for(i in 1:length(nu.try)) {
                       invisible(capture.output(
                               hqmsa.nu.cv.full[[i]] <- 
                                       cv.hqreg(X = X_train , 
                                                y = Y_train , 
                                                method = method , 
                                                tau = tau , 
                                                gamma = gamma , 
                                                lambda = lambda.try ,
                                                alpha = alpha , 
                                                preprocess =
                                                        "standardize" , 
                                                screen = "ASR" , 
                                                penalty.factor = 
                                                        1 / abs(best.ridge.coefs) ^ nu.try[i] , 
                                                FUN = "hqreg" , 
                                                type.measure = "mse"
                                                )
                                        )
                               )
                       hqmsa.nu.cv.mse[i] <-
                               min(hqmsa.nu.cv.full[[i]]$cve)
                       hqmsa.nu.cv.msesd[i] <-
                               hqmsa.nu.cv.full[[i]]$cvse[
                         which.min(hqmsa.nu.cv.full[[i]]$cve)
                                                         ]
                       hqmsa.nu.cv.lambda[i] <-
                               hqmsa.nu.cv.full[[i]]$lambda.min
                       hqmsa.nu.cv.coefs[[i]] <- 
                               hqmsa.nu.cv.full[[i]]$fit$beta[ , 
                         which.min(hqmsa.nu.cv.full[[i]]$cve)
                                                             ]
                       }
       }

       
       #specify minimizing nu value and resulting model info
       nu.opt <- nu.try[which.min(hqmsa.nu.cv.mse)]
       lambda.opt <- 
               hqmsa.nu.cv.lambda[
                       which.min(hqmsa.nu.cv.mse)
                       ]
       weights.opt <- 1 / abs(best.ridge.coefs) ^ nu.opt
       hqmsa.coefs <-
               hqmsa.nu.cv.coefs[[
                       which.min(hqmsa.nu.cv.mse)
                       ]]
       hqmsa.mse.min <- min(hqmsa.nu.cv.mse)
       if(!is.null(hqmsa.nu.cv.msesd[1])) {
               hqmsa.mse.min.se <- hqmsa.nu.cv.msesd[
                       which.min(hqmsa.nu.cv.mse)
                       ]               
       }

       hqmsa.model.min <- 
               hqmsa.nu.cv.full[
                       which.min(hqmsa.nu.cv.mse)
                       ]
       n.coefs <- sum(hqmsa.coefs[-1] != 0)
       
       # calculate metrics using holdout data, if applicable
       if(!is.null(X_test) & !is.null(Y_test)) {
               # store n
               n <- nrow(data.list[["X_test"]])
               
               # calculate predicted values
               y.pred <- data.list[["X_test"]] %*% hqmsa.coefs[-1]
               if(!is.na(hqmsa.coefs[1])) {
                       y.pred <- y.pred + hqmsa.coefs[1]
               }
   
               # calculate residual
               resid <- y.pred - Y_test
               
               # square the residuals
               resid.sq <- resid ^ 2
               
               # sum the square of residuals
               sum.resid.sq <- sum(resid.sq)
               
               #calculate root mse
               mse <- sum.resid.sq / n
               
               # set endpoint for timer
               end <- Sys.time()
               
               # temporarily store time of current model
               time <- abs(as.numeric(difftime(start , 
                                               end , 
                                               units = "secs"
                                               )
                                      )
                           )
               
               # print the total runtime of the current model
               if(print.time) {
                       cat("time = " , time , " \n ")
               }
               
               
               # put conditions, model info, and metrics into list
               return(list(full.model = hqmsa.model.min ,
                           model.info = list(lambda = lambda.opt , 
                                             coefs = hqmsa.coefs , 
                                             weights = weights.opt
                                             ) , 
                           metrics = list(n.coefs = n.coefs , 
                                          runtime = time , 
                                          mse = mse
                                          )
                           )
                      )
       } else {
               # set endpoint for timer
               end <- Sys.time()
               
               # temporarily store time of current model
               time <- abs(as.numeric(difftime(start , 
                                               end , 
                                               units = "secs"
                                               )
                                      )
                           )
               
               # print the total runtime of the current model
               if(print.time) {
                       cat("time = " , time , " \n ")
               }
               
               # put conditions, model info, and metrics into list
               return(list(full.model = hqmsa.model.min ,
                           model.info = list(lambda = lambda.opt , 
                                             coefs = hqmsa.coefs , 
                                             weights = weights.opt
                                             ) , 
                           metrics = list(n.coefs = n.coefs , 
                                          runtime = time
                                          )
                           )
                      )
       }
       
       

       

}
```

### Generating the models

#### Adaptive LAD Lasso

```{r ladlasso}
#run across data subset
ladlasso.100 <- wtcv.100 %>%
        map(safely(hqmsa.sim.fnct) , method = "LAD" , alpha = 1 , print.time = FALSE)
```

#### Adaptive LAD Elastic Net, Alpha = 0.5

```{r ladelnet5}
#run across data subset
ladelnet5.100 <- wtcv.100 %>%
        map(safely(hqmsa.sim.fnct) , method = "LAD" , alpha = 0.5 , print.time = FALSE)
```

#### Adaptive Huber Lasso

```{r huberlasso}
#run across data subset
huberlasso.100 <- wtcv.100 %>%
        map(safely(hqmsa.sim.fnct) , method = "huber" , alpha = 1 , print.time = FALSE)
```

#### Adaptive Huber Elastic Net, Alpha = 0.5

```{r huberelnet5}
#run across data subset
huberelnet5.100 <- wtcv.100 %>%
        map(safely(hqmsa.sim.fnct) , method = "huber" , alpha = 0.5)
```

#### Multi-Step Adaptive Elastic Net, k = 3, Alpha = 0.5

```{r msaenet k3}
#run across data subset
msaelnetk3.100 <- wtcv.100 %>%
        map(safely(hqmsa.sim.fnct) , method = "msaenet" , nsteps = 3L , print.time = FALSE)
```

#### Multi-Step Adaptive Elastic Net, k = 5, Alpha = 0.5

```{r msaenet k5}
#run across data subset
msaelnetk5.100 <- wtcv.100 %>%
        map(safely(hqmsa.sim.fnct) , method = "msaenet" , nsteps = 5L , print.time = FALSE)
```

#### Multi-Step Adaptive Elastic Net, k = 10, Alpha = 0.5

```{r msaenet k10}
#run across data subset
msaelnetk10.100 <- wtcv.100 %>%
        map(safely(hqmsa.sim.fnct) , method = "msaenet" , nsteps = 10L , print.time = FALSE)
```


### Processing the model results

Now we would like to process the resulting objects to eliminate any extra information. In this context, we're mainly interested in getting rid of the overarching 'result'/'error' list structure that is produced by **<span style = "color:blue">`` `r "safely()"` ``</span>**.

#### Get rid of result/error level of each model: Function

The following code chunk will check a data list produced by **<span style = "color:blue">`` `r "safely()"` ``</span>** and return either the 'result' element or the 'error' element, depending on which contains information.

```{r error or result function}
#eliminate result/error level
res_or_err <- function(data) {
        # if the 'error' list element is NULL, store the 'result' element contents
        if(is.null(data[["error"]])) {
                temp <- data[["result"]]
        } else { 
                # otherwise, store the 'error' element contents
                temp <- data[["error"]]
        }
        
        # save the resulting object
        return(temp)
}
```

#### Get rid of result/error and combine

Now, we're going to map **<span style = "color:blue">`` `r "res_or_err()"` ``</span>** across all of our 100-split model results and combine those objects into a single overarching list.

```{r no resulterror and combine models}
modelswt_all <- list(ladlasso = ladlasso.100 %>%
                       map(res_or_err) , 
                   ladelnet = ladelnet5.100 %>%
                       map(res_or_err) , 
                   huberlasso = huberlasso.100 %>%
                       map(res_or_err) , 
                   huberelnet = huberelnet5.100 %>%
                       map(res_or_err) , 
                   msak3 = msaelnetk3.100 %>%
                       map(res_or_err) , 
                   msak5 = msaelnetk5.100 %>%
                       map(res_or_err) ,
                   msak10 = msaelnetk10.100 %>%
                       map(res_or_err))
```


## Process Model Coefficients

See <span style = "color:green">`` `r "04B_Process_Coefficients_AllAdapts"` ``</span> ([Demo 4B](https://multach87.github.io/Robust_Lasso_ElasticNet/Component_Demos/04B_Process_Coefficients_AllAdapts.html)) for a more detailed walkthrough of this step, and <span style = "color:green">`` `r "04A_Process_Coefficients_SingleAdapt"` ``</span> ([Demo 4A](https://multach87.github.io/Robust_Lasso_ElasticNet/Component_Demos/04A_Process_Coefficients_SingleAdapt.html)) for an additional step of that component. I will be including the processing functions themselves in this walkthrough, but without the detailed explanation included in the component demonstrations.

### Coefficient frequency function

There are many pieces of information that might be extracted from such a rich collection of model results. Let's focus, however, on predictor selection/inclusion frequency.

```{r extract coefficient frequency information}
coefs_freq <- function(models) {
       # initialize vector of predictor selection frequency, with all values set to 0
       coefs.freq <- numeric(length = length(models[[1]][["model.info"]][["coefs"]][-1])
                             )
       
       # label frequency vector with predictor names, accounting for intercept
       names(coefs.freq) <- names(models[[1]][["model.info"]][["coefs"]][-1])
       
       # update each predictors' selection frequency if the corresponding coefficient value
       # # in each data split is nonzero (aka, was selected into the model)
       for(i in 1:length(models)) {
              for(j in 1:length(coefs.freq)) {
                     if(models[[i]][["model.info"]][["coefs"]][(j + 1)] != 0) {
                            coefs.freq[j] <- coefs.freq[j] + 1
                     }
              }
       }
       
       return(coefs.freq)
}
```

### Coefficient selection rank function

The function below calculates the selection frequency rank of each potential predictor.

```{r extract coefficient rank information}
coefs_rank <- function(freqs) {
       # initialize vector of selection rank, with all values set to NA
       coefs.rank <- numeric(length = length(freqs)
                             )
       
       coefs.rank <- frankv(as.numeric(freqs) , order = -1 , ties.method = "min")
       
       # label rank vector with predictor names, accounting for intercept
       names(coefs.rank) <- names(freqs)
       
       return(coefs.rank)       
}
```

### Minor functions: All/No Splits/Adaptations

I'd like to count the number of variables with $100\%$ or $0\%$ selection for each adaptation. I'm going to create two basic functions that I can apply during piping to generate these values.

```{r all_splits function}
all_splits <- function(data) {
        return(length(which(data[1:length(data)] == 100)
                      )
               )
}
```

```{r no_splits function}
no_splits <- function(data) {
        return(length(which(data[1:length(data)] == 0)  
                      )
        )
}
```

Similarly, I would like to calculate the number of adaptations which selected each variable $100\%$ and $0\%$ of the time.

```{r adapts_100 function}
adapts_100 <- function(data) {
        return(length(which(data[1:length(data)] == 100)  
                      )
               )
}
```

```{r adapts_0 function}
adapts_0 <- function(data) {
        return(length(which(data[1:length(data)] == 0) 
                      )
        )
}
```

### Coefficient processing: Initialize list

Let's initialize our coefficient selection frequency list element.

```{r make Frequencies list object in full model list}
modelswt_all[["Frequencies"]] <- list()
```

### Coefficient frequency processing: By Adaptation

Now let's map our coefficient frequency function across all adaptations and model results to create our first data.table. 

```{r run coef frequency function by adaptation}
modelswt_all[["Frequencies"]][["By_Adaptation"]] <- modelswt_all[1:7] %>%
       map(coefs_freq) %>%
       setDT %>%
       t %>%
       data.frame %>%
       setDT %>%
       .[ , "all" := apply(. , 1 , all_splits)] %>%
       .[ , "none" := apply(. , 1 , no_splits)] %>%
       .[ , "method" := names(modelswt_all[-8])] %>%
       setkey(. , method) 
```


### Coefficient frequency processing: By Potential Predictor

Now we're going to do the same thing, except by predictor instead of adaptation. 

```{r run coef frequency function by predictor}
modelswt_all[["Frequencies"]][["By_Predictor"]] <- modelswt_all[1:7] %>%
        map(coefs_freq) %>%
        data.frame %>%
        setDT %>%
        .[ , "all" := apply(. , 1 , adapts_100)] %>%
        .[ , "none" := apply(. , 1 , adapts_0)] %>%
        .[ , "predictor" := names(modelswt_all[["Frequencies"]][["By_Adaptation"]])[1:(ncol(modelswt_all[["Frequencies"]][["By_Adaptation"]]) - 3)]] %>%
        setkey(. , predictor) 
```

### Coefficient frequency processing - selection rank

Now we're going to generate the selection ranks of each potential predictor, _by adaptation_. This will only be by adaptation. 

```{r run coef rank function by adaptation}
modelswt_all[["Ranks"]] <- modelswt_all[["Frequencies"]][["By_Adaptation"]][ , 
                                                                         1:length(wt.data[["betas"]]) , 
                                                                         with = F] %>%
       apply(. , 1 , coefs_rank) %>%
       t %>%
       as.data.frame %>%
       setDT %>%
       cbind(. , modelswt_all[["Frequencies"]][["By_Adaptation"]][ , "method"])
```

## Save the Full Models File

Let's save the model results file which also includes our new "Frequencies" data.table so that we can load it during the full walkthrough.

```{r save frequencies file}
saveRDS(modelswt_all , "/Users/Matt/Desktop/GitHub_Dissertation_Scripts/Robust_Lasso_ElasticNet/Datasets/models100wt_plus_freqsranks.RData")
```

## Visualization

Please see <span style = "color:green">`` `r "05_Coefficient Visualization"` ``</span> ([Demo 5](https://multach87.github.io/Robust_Lasso_ElasticNet/Component_Demos/05_Coefficient Visualization.html)) for a more detailed walkthrough of this step

Now we're going to set up our visualizations. There are going to be three sets of plots:
  
  * Plots of selection frequencies of Top-X predictors by each adaptation
  * Plots of $100\%$ and $0\%$ selection by each adaptation
  * Plots of frequency of $100\%$ and $0\%$ selection of each predictor across adaptations
  
### Visualization Setup

#### Custom Color Palette

```{r make P11 custom color palette}
P11 <- c("#02AD24" , "#FF0000" , "#0000FF" , "#9A4D42" , 
         "#00FFBE" , "#FF00B6" , "#000033" , "#00FF00" ,
         "#FFD300" , "#009FFF" ,  "#783FC1")
```

```{r make P7 from P11}
P7 <- sample(x = P11 , size = 7 , replace = FALSE)
```

```{r make P8 from P11}
P8 <- sample(x = P11 , size = 8 , replace = FALSE)
```

#### Plot legend function

The **<span style = "color:blue">`` `r "0g_legend()"` ``</span>** function used to generate a plot legend object was adapted from the top-rated response to [this post](https://stackoverflow.com/questions/11883844/inserting-a-table-under-the-legend-in-a-ggplot2-histogram).

```{r legend function for multi-plots}
g_legend<-function(a.gplot){
       tmp <- ggplot_gtable(ggplot_build(a.gplot))
       leg <- which(sapply(tmp$grobs , function(x) x$name) == "guide-box")
       legend <- tmp$grobs[[leg]]
       return(legend)
       }
```

### Visualization: All Predictors and Adaptations

Now, I'm going to create 7 separate plots. The plots will present the selection frequencies of each potential predictor for each of the 7 lasso/elastic net adaptations included in the combined model.

#### Plot Objects

```{r huberelnet selection frequencies}
huberelnet.pl <- ggplot(data = modelswt_all[["Frequencies"]][["By_Predictor"]][ , c("huberelnet" , "predictor") , ] , 
                     aes(x = predictor , y = huberelnet)) +
  geom_col(aes(fill = predictor)) + 
  labs(title = "Adaptive Huber Elastic Net" ,
       y = "Frequency") + 
  theme(legend.position = "none" , 
        plot.title = element_text(hjust = 0.5 , 
                                  size = 8) , 
        axis.title.x = element_text(size = 8) , 
        axis.title.y = element_text(size = 8)) +
  scale_fill_manual(values = P8)
```

```{r huberlasso selection frequencies}
huberlasso.pl <- ggplot(data = modelswt_all[["Frequencies"]][["By_Predictor"]][ , c("huberlasso" , "predictor") , ] , 
                     aes(x = predictor , y = huberlasso)) +
  geom_col(aes(fill = predictor)) + 
  labs(title = "Adaptive Huber Lasso" ,
       y = "Frequency") + 
  theme(legend.position = "none" , 
        plot.title = element_text(hjust = 0.5 , 
                                  size = 8) , 
        axis.title.x = element_text(size = 8) , 
        axis.title.y = element_text(size = 8)) +
  scale_fill_manual(values = P8)
```

```{r ladelnet selection frequencies}
ladelnet.pl <- ggplot(data = modelswt_all[["Frequencies"]][["By_Predictor"]][ , c("ladelnet" , "predictor") , ] , 
                     aes(x = predictor , y = ladelnet)) +
  geom_col(aes(fill = predictor)) + 
  labs(title = "Adaptive LAD Elastic Net" ,
       y = "Frequency") + 
  theme(legend.position = "none" , 
        plot.title = element_text(hjust = 0.5 , 
                                  size = 8) , 
        axis.title.x = element_text(size = 8) , 
        axis.title.y = element_text(size = 8)) +
  scale_fill_manual(values = P8)
```

```{r ladlasso selection frequencies}
ladlasso.pl <- ggplot(data = modelswt_all[["Frequencies"]][["By_Predictor"]][ , c("ladlasso" , "predictor") , ] , 
                     aes(x = predictor , y = ladlasso)) +
  geom_col(aes(fill = predictor)) + 
  labs(title = "Adaptive LAD Lasso" ,
       y = "Frequency") + 
  theme(legend.position = "none" , 
        plot.title = element_text(hjust = 0.5 , 
                                  size = 8) , 
        axis.title.x = element_text(size = 8) , 
        axis.title.y = element_text(size = 8)) +
  scale_fill_manual(values = P8)
```

```{r msak10 selection frequencies}
msak10.pl <- ggplot(data = modelswt_all[["Frequencies"]][["By_Predictor"]][ , c("msak10" , "predictor") , ] , 
                     aes(x = predictor , y = msak10)) +
  geom_col(aes(fill = predictor)) + 
  labs(title = "10-Step Adaptive Elastic Net" ,
       y = "Frequency") + 
  theme(legend.position = "none" , 
        plot.title = element_text(hjust = 0.5 , 
                                  size = 8) , 
        axis.title.x = element_text(size = 8) , 
        axis.title.y = element_text(size = 8)) +
  scale_fill_manual(values = P8)
```

```{r msak3 selection frequencies}
msak3.pl <- ggplot(data = modelswt_all[["Frequencies"]][["By_Predictor"]][ , c("msak3" , "predictor") , ] , 
                     aes(x = predictor , y = msak3)) +
  geom_col(aes(fill = predictor)) + 
  labs(title = "3-Step Adaptive Elastic Net" ,
       y = "Frequency") + 
  theme(legend.position = "none" , 
        plot.title = element_text(hjust = 0.5 , 
                                  size = 8) , 
        axis.title.x = element_text(size = 8) , 
        axis.title.y = element_text(size = 8)) +
  scale_fill_manual(values = P8)
```

```{r msak5 selection frequencies}
msak5.pl <- ggplot(data = modelswt_all[["Frequencies"]][["By_Predictor"]][ , c("msak5" , "predictor") , ] , 
                     aes(x = predictor , y = msak5)) +
  geom_col(aes(fill = predictor)) + 
  labs(title = "5-Step Adaptive Elastic Net" ,
       y = "Frequency") + 
  theme(legend.position = "none" , 
        plot.title = element_text(hjust = 0.5 , 
                                  size = 8) , 
        axis.title.x = element_text(size = 8) , 
        axis.title.y = element_text(size = 8)) +
  scale_fill_manual(values = P8)
```

#### Blank Grob

```{r blank ggplot}
blank.pl <- ggplot() +
  geom_blank() + 
  theme_minimal()
```

#### Create Plot Legend

```{r plot object for first multiplot legend}
msak5.l <- ggplot(data = modelswt_all[["Frequencies"]][["By_Predictor"]][ , c("msak5" , "predictor") , ] , 
                     aes(x = predictor , y = msak5)) +
  geom_col(aes(fill = predictor)) + 
  labs(title = "5-Step Adaptive Elastic Net" ,
       y = "Frequency") + 
  theme(plot.title = element_text(hjust = 0.5 , 
                                  size = 8) , 
        axis.title.x = element_text(size = 8) , 
        axis.title.y = element_text(size = 8)) +
  scale_fill_manual(values = P8)
```

```{r legend for first multiplot}
plotA.legend <- g_legend(msak5.l)
```

#### Put Plot Objects Together

Let's put the plots together!
 

```{r multiplotA arrange}
plotA <- grid.arrange(arrangeGrob(huberelnet.pl , blank.pl , huberlasso.pl , 
                                  ladelnet.pl , blank.pl , ladlasso.pl , 
                                  msak3.pl , msak5.pl , msak10.pl , 
                                  nrow = 3) , 
                       arrangeGrob(plotA.legend) , 
                      ncol = 2 , widths = c(200 , 40) , 
                      top = textGrob("Predictor Selection Frequency by Lasso/Elastic Net Adaptation Across All 100 Data Splits"))
```

### Visualization: All/None Predictor Selection by Adaptation

#### Plot Objects

Now, I'm going to create 2 separate plots. One plot will display the number of predictors selected into $100\%$ of models by each of the 7 adaptations, while the second plot will display similar information for number of predictors selected into *no* models by adaptation. We can mostly just take our previous code and make slight changes, but the structure of creating these objects is largely the same.

```{r adaptation 100 plot}
adapt100.pl <- ggplot(data = modelswt_all[["Frequencies"]][["By_Adaptation"]][ , c("all" , "method") , ] , 
                     aes(x = method , y = all)) +
  geom_col(aes(fill = method)) + 
  labs(title = "100% Selection" ,
       y = "100% Selection (out of 7)" , 
       x = "Adaptation") + 
  coord_cartesian(ylim = c(0 , 5)) + 
  theme(legend.position = "none" , 
        plot.title = element_text(hjust = 0.5 , 
                                  size = 8) , 
        axis.text.x = element_text(hjust = 1 , 
                                   vjust = 1 , 
                                   angle = 45)) +
  scale_fill_manual(values = P7)
```

```{r adaptation 0 plot}
adapt0.pl <- ggplot(data = modelswt_all[["Frequencies"]][["By_Adaptation"]][ , c("none" , "method") , ] , 
                     aes(x = method , y = none)) +
  geom_col(aes(fill = method)) + 
  labs(title = "0% Selection" ,
       y = "0% Selection (out of 8)" , 
       x = "Adaptation") + 
  coord_cartesian(ylim = c(0 , 5)) + 
  theme(legend.position = "none" , 
        plot.title = element_text(hjust = 0.5 , 
                                  size = 8) , 
        axis.text.x = element_text(hjust = 1 , 
                                   vjust = 1 , 
                                   angle = 45)) +
  scale_fill_manual(values = P7)
```

#### Create Plot Legend

Let's also create the legend for this multi-plot after creating an analogous plot with the legend included.

```{r plot object for multiplotB legend}
adapt100.l <- ggplot(data = modelswt_all[["Frequencies"]][["By_Adaptation"]][ , c("all" , "method") , ] , 
                     aes(x = method , y = all)) +
  geom_col(aes(fill = method)) + 
  labs(title = "100% Selection" ,
       y = "100% Selection (out of 8)" , 
       x = "Adaptation") + 
  coord_cartesian(ylim = c(0 , 5)) + 
  theme(plot.title = element_text(hjust = 0.5 , 
                                  size = 8) , 
        axis.text.x = element_text(angle = 45)) +
  scale_fill_manual(values = P7)
```

```{r legend for multiplotB}
plotB.legend <- g_legend(adapt100.l)
```


#### Put Plot Objects Together

```{r multiplotB arrange}
plotB <- grid.arrange(arrangeGrob(adapt0.pl , 
                                  adapt100.pl , 
                                  nrow = 2) , 
                       arrangeGrob(plotB.legend) , 
                      ncol = 2 , widths = c(200 , 40) , 
                      top = textGrob("0% and 100% Selection Frequency by Lasso/Elastic Net Adaptation"))
```

### Visualization: All/None Predictor Selection by Predictor

I'm going to create analogous plots to those in the previous subsection. This time, however, the plots look at the number of adaptations for which a given predictor was _always_ or _never_ selected across the 100 data splits. 

#### Plot Objects

```{r predictor 100 plot}
pred100.pl <- ggplot(data = modelswt_all[["Frequencies"]][["By_Predictor"]][ , c("all" , "predictor") , ] , 
                     aes(x = predictor , y = all)) +
  geom_col(aes(fill = predictor)) + 
  labs(title = "100% Selection" ,
       y = "100% Selection (out of 8 possible)" , 
       x = "Predictor") + 
  coord_cartesian(ylim = c(0 , 5)) + 
  theme(legend.position = "none" , 
        plot.title = element_text(hjust = 0.5 , 
                                  size = 8) , 
        axis.text.x = element_text(angle = 45)) +
  scale_fill_manual(values = P8)
```

```{r predictor 0 plot}
pred0.pl <- ggplot(data = modelswt_all[["Frequencies"]][["By_Predictor"]][ , c("none" , "predictor") , ] , 
                     aes(x = predictor , y = none)) +
  geom_col(aes(fill = predictor)) + 
  labs(title = "0% Selection" ,
       y = "0% Selection (out of 8 possible)" , 
       x = "Predictor") + 
  coord_cartesian(ylim = c(0 , 5)) + 
  theme(legend.position = "none" , 
        plot.title = element_text(hjust = 0.5 , 
                                  size = 8) , 
        axis.text.x = element_text(angle = 45)) +
  scale_fill_manual(values = P8)
```

#### Create Plot Legend

Let's also create the legend for this multi-plot after creating an analogous plot with the legend included.

```{r plot object for multiplotC}
pred100.l <- ggplot(data = modelswt_all[["Frequencies"]][["By_Predictor"]][ , c("all" , "predictor") , ] , 
                     aes(x = predictor , y = all)) +
  geom_col(aes(fill = predictor)) + 
  labs(title = "100% Selection" ,
       y = "100% Selection (out of 7)" , 
       x = "Predictor") + 
  coord_cartesian(ylim = c(0 , 5)) + 
  theme(plot.title = element_text(hjust = 0.5 , 
                                  size = 8)) +
  scale_fill_manual(values = P8)
```

```{r legend for multiplotC legend}
plotC.legend <- g_legend(pred100.l)
```


#### Put Plot Objects Together

```{r multiplotC arrange}
plotC <- grid.arrange(arrangeGrob(pred0.pl , 
                                  pred100.pl , 
                                  nrow = 2) , 
                       arrangeGrob(plotB.legend) , 
                      ncol = 2 , widths = c(200 , 40) , 
                      top = textGrob("0% and 100% Selection Frequency by Potential Predictor"))
```

## References
